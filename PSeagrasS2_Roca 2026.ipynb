{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marroca13/PSeagrasS2/blob/main/PSeagrasS2_Roca%202026.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n5oh5WCsRYn"
      },
      "source": [
        "# **PSeagrasS2: Blending PlanetScope and Sentinel-2 imagery for seagrass change detection in Google Earth Engine**\n",
        "*Diclaimer: your imagery has to be atmospherically corrected, thats why we integrate ACOLITE processor into the workflow (default settings, v20231023.0). If prefered, you can also use your corrected imagery avoiding step 1 and importing them to Drive or GEE as an asset.*\n",
        "\n",
        "*This code has been developed for the eutrophic Lagoa da ConceiÃ§ao in Florianopolis (Brazil).*\n",
        "\n",
        "---\n",
        "\n",
        "The code includes:\n",
        "1. Sentinel-2 atmospheric correction: L1C to L2A using ACOLITE inside GEE\n",
        "2. ODW and land adjacent pixels masking\n",
        "3. Co-registration of Sentinel-2 images into PlanetScope grid\n",
        "4. Preparation of in situ data: training and validation sets\n",
        "5. Depth invariant index and band ratios calculation over Sentinel-2\n",
        "6. Random Forest: binary classification\n",
        "  6.1. Pre-event + uncertainty assessment\n",
        "  6.2. Post-event + uncertainty assessment\n",
        "7. Seagrass change quantification\n",
        "\n",
        "\n",
        "> **Mar Roca Mora**\n",
        "Last update: 2025/06/11\n",
        "mar.roca@csic.es\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYnUwfkbglaD"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "817qxsc-gpA8"
      },
      "outputs": [],
      "source": [
        "!pip install xlsxwriter\n",
        "!pip install geemap\n",
        "!pip install netCDF4\n",
        "!pip install rioxarray\n",
        "!pip install earthengine-api\n",
        "!pip install scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHJSLzJ2unAe"
      },
      "source": [
        "# 0 - Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPzi0bSmj5TQ"
      },
      "outputs": [],
      "source": [
        "## Connection to GEE project\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='your-project') #set your GEE project here\n",
        "\n",
        "## Connection to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0FbtfXvVczFD"
      },
      "outputs": [],
      "source": [
        "## Import some libraries\n",
        "import os\n",
        "import ee\n",
        "import geemap\n",
        "import geemap.colormaps as cm\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import numpy as np\n",
        "import rioxarray\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16aOvtCMj0Bk"
      },
      "outputs": [],
      "source": [
        "geemap.ee_initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUAmKPh3qa_r"
      },
      "source": [
        "\n",
        "\n",
        "# 1 - Atmospheric correction - ACOLITE inside GEE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "j0zEmG9FrAHQ"
      },
      "outputs": [],
      "source": [
        "# Set directory\n",
        "%cd '/content/drive/My Drive/ACOLITE/' ## set the folder where you have ACOLITE in Google Drive + the ACOLITE file that connects to GEE\n",
        "\n",
        "# Verify directory\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs2CCtMEr7Tg"
      },
      "outputs": [],
      "source": [
        "## Connect to ACOLITE version and import (takes aroud 1 minute)\n",
        "import sys\n",
        "ACOLITE_PATH = '/content/drive/My Drive/ACOLITE/20231023.0' # ACOLITE version itself\n",
        "sys.path.append(ACOLITE_PATH)\n",
        "import ACOLITE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6RNYZsSAvXa"
      },
      "source": [
        "## ACOLITE functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxYnvjFECdPK"
      },
      "outputs": [],
      "source": [
        "def correct_l1s(collection):\n",
        "  collection_list = collection.toList(collection.size().getInfo())\n",
        "  images = []\n",
        "  for i in range(collection_list.size().getInfo()):\n",
        "    image = ee.Image(collection_list.get(i))\n",
        "    images.append(ACOLITE.dask_spectrum_fitting(image)[0])\n",
        "\n",
        "  return ee.ImageCollection.fromImages(images)\n",
        "\n",
        "def water_quality(collection, products):\n",
        "  for product in products:\n",
        "    collection = collection.map(product)\n",
        "  return collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUZW1Xq3AlfP"
      },
      "outputs": [],
      "source": [
        "## Sentinel-2 imagery: from L1C to L2A using ACOLITE integrated into GEE\n",
        "roi = ee.FeatureCollection('projects/ee-seagrass-mar/assets/Brazil/Lagoa_sections').geometry()\n",
        "\n",
        "## Pre-event (2018) selected Sentinel-2 images:\n",
        "pre1 = 'COPERNICUS/S2_HARMONIZED/20180308T131241_20180308T131717_T22JGQ'\n",
        "pre2 = 'COPERNICUS/S2_HARMONIZED/20180316T132229_20180316T132601_T22JGQ'\n",
        "pre3 = 'COPERNICUS/S2_HARMONIZED/20180425T132229_20180425T132231_T22JGQ'\n",
        "\n",
        "sentinel2_l1c_pre = ee.ImageCollection([\n",
        "    ee.Image(pre1),\n",
        "    ee.Image(pre2),\n",
        "    ee.Image(pre3)\n",
        "])\n",
        "\n",
        "# Convert DN to rrs and downscale to 10 meters\n",
        "sentinel2_l1r_rrs_pre = sentinel2_l1c_pre.map(ACOLITE.select_sentinel2_bands).map(ACOLITE.to_rrs).map(ACOLITE.to_10m)\n",
        "sentinel2_list = sentinel2_l1r_rrs_pre.toList(sentinel2_l1r_rrs_pre.size())\n",
        "# Apply atmospheric correction Dark Spectrum Fitting\n",
        "sentinel2_l2r_rrs_pre = correct_l1s(sentinel2_l1r_rrs_pre)\n",
        "# Remove sunglint effect\n",
        "sentinel2_l2r_rrs_pre_deglint = ee.ImageCollection.fromImages( [ ACOLITE.deglint_alternative(*ACOLITE.dask_spectrum_fitting( ee.Image(sentinel2_list.get(i)))) for i in range(sentinel2_l2r_rrs_pre.size().getInfo()) ] )\n",
        "# Calculate water quality parameters of interest. In this case: chlorophyll-a and Suspended Particulate Matter (SPM)\n",
        "sentinel2_l2w_rrs_pre = water_quality(sentinel2_l2r_rrs_pre_deglint, [ACOLITE.add_chl_re_mishra, ACOLITE.add_spm_nechad2016_665])\n",
        "\n",
        "# Post-event (2024) selected Sentinel-2 images\n",
        "post1 = 'COPERNICUS/S2_HARMONIZED/20240311T131239_20240311T131237_T22JGQ'\n",
        "post2 = 'COPERNICUS/S2_HARMONIZED/20240331T131239_20240331T131627_T22JGQ'\n",
        "post3 = 'COPERNICUS/S2_HARMONIZED/20240410T131239_20240410T131751_T22JGQ'\n",
        "\n",
        "sentinel2_l1c_post = ee.ImageCollection([\n",
        "    ee.Image(post1),\n",
        "    ee.Image(post2),\n",
        "    ee.Image(post3)\n",
        "])\n",
        "\n",
        "# Convert DN to rrs and downscale to 10 meters\n",
        "sentinel2_l1r_rrs_post = sentinel2_l1c_post.map(ACOLITE.select_sentinel2_bands).map(ACOLITE.to_rrs).map(ACOLITE.to_10m)\n",
        "sentinel2_list = sentinel2_l1r_rrs_post.toList(sentinel2_l1r_rrs_post.size())\n",
        "# Apply atmospheric correction Dark Spectrum Fitting\n",
        "sentinel2_l2r_rrs_post = correct_l1s(sentinel2_l1r_rrs_post)\n",
        "# Remove sunglint effect\n",
        "sentinel2_l2r_rrs_post_deglint = ee.ImageCollection.fromImages( [ ACOLITE.deglint_alternative(*ACOLITE.dask_spectrum_fitting( ee.Image(sentinel2_list.get(i)))) for i in range(sentinel2_l2r_rrs_pre.size().getInfo()) ] )\n",
        "# Calculate water quality parameters of interest. In this case: chlorophyll-a and Suspended Particulate Matter (SPM)\n",
        "sentinel2_l2w_rrs_post = water_quality(sentinel2_l2r_rrs_post_deglint, [ACOLITE.add_chl_re_mishra, ACOLITE.add_spm_nechad2016_665])\n",
        "\n",
        "print('Proccessed!')\n",
        "print('Number of images processed pre-event:', sentinel2_l2w_rrs_pre.size().getInfo())\n",
        "print('Number of images processed post-event:', sentinel2_l2w_rrs_post.size().getInfo())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvY_7TjOEgdK"
      },
      "outputs": [],
      "source": [
        "## A 'constant' band is automatically generated. Remove it from the Image Collections\n",
        "band_names = sentinel2_l2w_rrs_pre.first().bandNames()\n",
        "\n",
        "# Select all bands except the constant band\n",
        "sentinel2_l2w_rrs_pre = sentinel2_l2w_rrs_pre.map(lambda image: image.select(band_names.filter(ee.Filter.neq('item', 'constant'))))\n",
        "sentinel2_l2w_rrs_post = sentinel2_l2w_rrs_post.map(lambda image: image.select(band_names.filter(ee.Filter.neq('item', 'constant'))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZzlIdj7HiD3"
      },
      "outputs": [],
      "source": [
        "## Clip Image Collections\n",
        "# pre-event\n",
        "sentinel2_l2w_rrs_pre = sentinel2_l2w_rrs_pre.map(lambda image: image.clip(roi.dissolve()))\n",
        "# post-event\n",
        "sentinel2_l2w_rrs_post = sentinel2_l2w_rrs_post.map(lambda image: image.clip(roi.dissolve()))\n",
        "\n",
        "first_image = sentinel2_l2w_rrs_pre.first()\n",
        "band_names = first_image.bandNames()\n",
        "\n",
        "print(band_names.getInfo())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LRzJ3inDYCa"
      },
      "outputs": [],
      "source": [
        "# This method can also be used for a multi-temporal pre-stack approach\n",
        "\n",
        "# # Range of images used for the multi-temporal composite pre-processing stack\n",
        "# start = '2018-02-01'\n",
        "# end = '2018-04-30'\n",
        "# tile = '22JGQ'\n",
        "# bounds = [-48.490219, -27.634741, -48.423615, -27.509432]\n",
        "\n",
        "# sentinel2_l1c_pre_multitemporal = ACOLITE.search_with_clouds(roi, start, end, tile = tile)\n",
        "\n",
        "## APPLY REDUCER\n",
        "# reduce(ee.Reducer.percentile([ee.Number(20))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEqBaAA26f2N"
      },
      "source": [
        "**01 - Set up PlanetScope imagery as a loaded GEE asset**\n",
        "\n",
        "*If the area has more than one tile, merge first through ACOLITE using merge_tiles = TRUE*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NiCk0jUBN0t"
      },
      "outputs": [],
      "source": [
        "## Import PlanetScope Imagery already processed and uploaded as an asset\n",
        "\n",
        "## If you have a Planet account linked to GEE, you can process everything on the cloud\n",
        "planetScope18 = ee.Image('projects/ee-seagrass-mar/assets/Brazil/PlanetScope_2018') # 4-bands 2259x4305 px\n",
        "bands18 = ['B1', 'B2', 'B3', 'B4', 'Kd_505', 'KPAR']\n",
        "planetScope18 = planetScope18.select(planetScope18.bandNames()).rename(['B1', 'B2', 'B3', 'B4', 'flags', 'Kd_505', 'KPAR']).select(bands18)\n",
        "print(planetScope18.bandNames().getInfo())\n",
        "\n",
        "\n",
        "planetScope24 = ee.Image('projects/ee-seagrass-mar/assets/Brazil/PlanetScope_2024') # 8-bands 2259x4305 px\n",
        "bands24 = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'Kd_492', 'KPAR'] # ['B2', 'B4', 'B6', 'B8', 'Kd_492', 'KPAR']\n",
        "planetScope24 = planetScope24.select(planetScope24.bandNames()).rename(['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'flags', 'Kd_492', 'KPAR']).select(bands24)\n",
        "print(planetScope24.bandNames().getInfo())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-C_AY6fWkAlf"
      },
      "outputs": [],
      "source": [
        "# Display imagery in map\n",
        "rgb_plot_s2 = {\n",
        "    'bands': ['B4', 'B3', 'B2'],\n",
        "    'gamma': 1.4,\n",
        "    'min': -0.0015375894198130027,\n",
        "    'max': 0.0883948298123742}\n",
        "\n",
        "rgb_plot_ps = {\n",
        "    'bands': ['B3', 'B2', 'B1'],\n",
        "    'gamma': 1.4,\n",
        "    'min': -0.003069632846763851,\n",
        "    'max': 0.023891219740272425}\n",
        "\n",
        "rgb_plot_ps_24 = {\n",
        "    'bands': ['B6', 'B4', 'B2'],\n",
        "    'min': 0.0017,\n",
        "    'max': 0.031,\n",
        "    'gamma': 1.4\n",
        "}\n",
        "\n",
        "Map = geemap.Map()\n",
        "Map.addLayer(sentinel2_l2w_rrs_post.first(), rgb_plot_s2, \"Sentinel-2 L2\")\n",
        "Map.addLayer(planetScope18, rgb_plot_ps, \"PlanetScope 2018\")\n",
        "Map.addLayer(planetScope24, rgb_plot_ps_24, \"PlanetScope 2024\")\n",
        "# Map.addLayer(sentinel2_l2r_rrs_deglint.first(), rgb_plot, \"Sentinel-2 L2 Deglint RGB\")\n",
        "# Map.addLayer(sentinel2_l2w_rrs.first(), rgb_plot, \"Sentinel-2 L2W RGB\")\n",
        "# Map.addLayer(sentinel2_l2w_rrs.first().select(f'TUR_Nechad2016_665'), tur_plot, \"Sentinel-2 Turbidity\")\n",
        "# Map.add_colorbar(tur_plot, label=\"Turbidity (NTU)\")\n",
        "Map.centerObject(planetScope18, 11)\n",
        "Map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edq_e0zpv3Mc"
      },
      "source": [
        "Test to export image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7E9VkCkPJ9Z"
      },
      "outputs": [],
      "source": [
        "# # Export to Drive\n",
        "# task = ee.batch.Export.image.toDrive(\n",
        "#     image=sentinel2_l2w_rrs_post.first(),\n",
        "#     description='S2L2A_post',\n",
        "#     region=roi.dissolve(),\n",
        "#     folder= \"/content/drive/My Drive/Colab_Notebooks/export\",\n",
        "#     fileFormat = \"GeoTIFF\",\n",
        "#      maxPixels = 1e13\n",
        "# )\n",
        "\n",
        "# task.start()\n",
        "# print('Exported!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzS6h86aF0yr"
      },
      "source": [
        "\n",
        "# 2 - Optically deep water (ODW) and land pixel masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aonSuSYcrF7K"
      },
      "outputs": [],
      "source": [
        "# For the ODW masking we used a delimitation from 1-meter secchi disk for 2024 processed for Sentinel-2 imagery.\n",
        "OSW = ee.FeatureCollection('projects/ee-seagrass-mar/assets/Brazil/OSW_zSD_new')\n",
        "bathy = ee.Image('projects/ee-seagrass-mar/assets/Brazil/Batimetria_Lagoa')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVKbiwkoJ2rj"
      },
      "outputs": [],
      "source": [
        "ndwi24 = planetScope24.normalizedDifference(['B8', 'B4'])\n",
        "land_mask = ndwi24.lt(0.1)\n",
        "\n",
        "Map.addLayer(land_mask, {'palette': 'orange'}, \"Land Mask 24\")\n",
        "# Map.addLayer(OSW, {'color': 'yellow'}, \"OSW\")\n",
        "# Map.centerObject(land_mask, 11)\n",
        "# Map\n",
        "\n",
        "zSD = ee.Image('projects/ee-seagrass-mar/assets/Brazil/PlanetScope_24').select('zSD')\n",
        "zSD = zSD.updateMask(zSD.lte(1))\n",
        "land_mask = ndwi24.lt(0.1).And(zSD.lte(1))\n",
        "Map.addLayer(land_mask, {'palette': 'orange'}, \"Shallow water mask\")\n",
        "# Map.centerObject(land_mask, 11)\n",
        "# Map\n",
        "\n",
        "Map.addLayer(zSD, {\"min\":0,\"max\":1,\"palette\":[\"e0f7fa\",\"b2ebf2\",\"81d4fa\",\"4fc3f7\",\"29b6f6\",\"039be5\",\"0288d1\",\"0277bd\",\"01579b\",\"003f5c\"]}, \"zSD\")\n",
        "Map.centerObject(zSD, 11)\n",
        "Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "si6OgmRKlzVs"
      },
      "outputs": [],
      "source": [
        "# # NDWI 0 threshold for the period January-March 2024, as water clarity was higher before the wastewater event\n",
        "# land_mask = ee.Image('projects/ee-seagrass-mar/assets/Brazil/NDWI_0_mask') # mask with PlanetScope\n",
        "\n",
        "# Map.addLayer(land_mask, {'palette': 'blue'}, \"NDWI mask\")\n",
        "# Map.centerObject(land_mask, 11)\n",
        "# Map\n",
        "\n",
        "# # JavaScript code: https://code.earthengine.google.com/89fef0cbd1accff4393d0de9019c9752"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCP-FNQYfmnj"
      },
      "outputs": [],
      "source": [
        "# Export to Drive\n",
        "task = ee.batch.Export.image.toDrive(\n",
        "    image=land_mask.clip(OSW),\n",
        "    description='mask',\n",
        "    region=roi.dissolve(),\n",
        "    folder= \"/content/drive/My Drive/Colab_Notebooks/export\",\n",
        "    fileFormat = \"GeoTIFF\",\n",
        "     maxPixels = 1e13\n",
        ")\n",
        "\n",
        "task.start()\n",
        "print('Exported!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArNxpV7aszp1"
      },
      "outputs": [],
      "source": [
        "sentinel2_l2w_rrs_pre = sentinel2_l2w_rrs_pre.map(lambda image: image.updateMask(land_mask).clip(OSW))\n",
        "sentinel2_l2w_rrs_pre = sentinel2_l2w_rrs_pre.map(lambda image: image.select(['B1','B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'chl_re_mishra', 'SPM_Nechad2016_665']))\n",
        "                                                  #.select(['B1','B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B10', 'B11', 'B12', 'chl_re_mishra', 'SPM_Nechad2016_665']))\n",
        "band_names = sentinel2_l2w_rrs_pre.first().bandNames()\n",
        "\n",
        "sentinel2_l2w_rrs_post = sentinel2_l2w_rrs_post.map(lambda image: image.updateMask(land_mask).clip(OSW))\n",
        "sentinel2_l2w_rrs_post = sentinel2_l2w_rrs_post.map(lambda image: image.select(['B1','B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'chl_re_mishra', 'SPM_Nechad2016_665']))\n",
        "                                                    #.select(['B1','B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B10', 'B11', 'B12', 'chl_re_mishra', 'SPM_Nechad2016_665']))\n",
        "\n",
        "planetScope18 = planetScope18.clip(OSW).updateMask(land_mask).select(['B1', 'B2', 'B3', 'B4', 'Kd_505', 'KPAR']) ## try remove B4 (NIR)\n",
        "planetScope24 = planetScope24.clip(OSW).updateMask(land_mask).select(['B1','B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'Kd_492', 'KPAR']) ## try remove B8 (NIR)\n",
        "\n",
        "rgb_plot_s2 = {\n",
        "    'bands': ['B4', 'B3', 'B2'],\n",
        "    'min': 0,\n",
        "    'max': 0.5,\n",
        "    'gamma': 1.4\n",
        "}\n",
        "\n",
        "\n",
        "Map.addLayer(sentinel2_l2w_rrs_pre.first(), rgb_plot_s2, \"S2 OSW\")\n",
        "Map.centerObject(land_mask, 11)\n",
        "Map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilbT_ecVWRGi"
      },
      "source": [
        "\n",
        "# 3 - Image co-registration\n",
        "*1 image of Sentinel-2 vs 1 image of PlanetScope*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeFvaX1RRL9a"
      },
      "source": [
        "3.1 - PlanetScope Classic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPou_urNwjwl"
      },
      "outputs": [],
      "source": [
        "# Reproject Sentinel-2 image to match PlanetScope 3-meter spatial resolution\n",
        "## PRE\n",
        "planet18_proj = planetScope18.projection()\n",
        "print(planet18_proj.getInfo())\n",
        "\n",
        "# Check scale\n",
        "scale = planet18_proj.nominalScale()\n",
        "print(f\"Spatial resolution of planetScope18: {scale.getInfo()} meters\")\n",
        "\n",
        "def reproject_and_rename(image):\n",
        "    # Resample, clip, and reproject\n",
        "    resampled_image = image.resample(\"bilinear\").reproject(\n",
        "        crs=planet18_proj.crs(), scale=planet18_proj.nominalScale()\n",
        "    )\n",
        "    # Rename bands\n",
        "    s2_prefix = 'S2_'\n",
        "    renamed_image = resampled_image.rename(resampled_image.bandNames().map(lambda band: ee.String(s2_prefix).cat(band)))\n",
        "    return renamed_image\n",
        "\n",
        "sentinel2_resampled_collection = sentinel2_l2w_rrs_pre.map(reproject_and_rename)\n",
        "\n",
        "# Rename bands for PlanetScope\n",
        "ps_prefix = 'PS_'\n",
        "ps_renamed = planetScope18.rename(planetScope18.bandNames().map(lambda band: ee.String(ps_prefix).cat(band)))\n",
        "\n",
        "# Combine PlanetScope and Sentinel-2 data\n",
        "def combine_images(image):\n",
        "    return ps_renamed.addBands(image)\n",
        "\n",
        "multi_sensor_pre = sentinel2_resampled_collection.map(combine_images).map(lambda image: image.updateMask(land_mask).clip(OSW))\n",
        "\n",
        "# Print band names for the first image in the collection\n",
        "first_image = multi_sensor_pre.first()\n",
        "band_names = first_image.bandNames()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBGOP0-hRSWx"
      },
      "source": [
        "3.2 - PlanetScope SuperDove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SihRmXWcsQo0"
      },
      "outputs": [],
      "source": [
        "# Reproject Sentinel-2 image to match PlanetScope 3-meter spatial resolution\n",
        "## POST\n",
        "planet24_proj = planetScope24.projection()\n",
        "print(planet24_proj.getInfo())\n",
        "\n",
        "# Check scale\n",
        "scale = planet24_proj.nominalScale()\n",
        "print(f\"Spatial resolution of planetScope18: {scale.getInfo()} meters\")\n",
        "\n",
        "def reproject_and_rename(image):\n",
        "    # Resample, clip, and reproject\n",
        "    resampled_image = image.resample(\"bilinear\").reproject(\n",
        "        crs=planet24_proj.crs(), scale=planet24_proj.nominalScale()\n",
        "    )\n",
        "    # Rename bands\n",
        "    s2_prefix = 'S2_'\n",
        "    renamed_image = resampled_image.rename(resampled_image.bandNames().map(lambda band: ee.String(s2_prefix).cat(band)))\n",
        "    return renamed_image\n",
        "\n",
        "sentinel2_resampled_collection = sentinel2_l2w_rrs_post.map(reproject_and_rename)\n",
        "\n",
        "# Rename bands for PlanetScope\n",
        "ps_prefix = 'PS_'\n",
        "ps_renamed = planetScope24.rename(planetScope24.bandNames().map(lambda band: ee.String(ps_prefix).cat(band)))\n",
        "\n",
        "# Combine PlanetScope and Sentinel-2 data\n",
        "def combine_images(image):\n",
        "    return ps_renamed.addBands(image)\n",
        "\n",
        "multi_sensor_post = sentinel2_resampled_collection.map(combine_images).map(lambda image: image.updateMask(land_mask).clip(OSW))\n",
        "\n",
        "# Print band names for the first image in the collection\n",
        "print(multi_sensor_post.first().bandNames().getInfo())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upgqHAHL0EZy"
      },
      "source": [
        "# 4 - Preparation of *in situ* data: training and validation sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooqPfZzr_8xR"
      },
      "source": [
        "Field sample data have the following classes as 'Bottom':\n",
        "\n",
        " 0 - seagrass (Halodule or Ruppia)\n",
        "\n",
        " 1 - nonSeagrass (sand, mud, rock, algae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRh2MbGO0LwF"
      },
      "outputs": [],
      "source": [
        "## Import in situ data for sand class selection\n",
        "# in situ 2018 has only seagrass & sand values\n",
        "\n",
        "# You can now use insitu2018_filtered for further analysis\n",
        "# Map.addLayer(insitu2018_filtered, {'color': 'red'}, 'in situ 2018 Filtered')\n",
        "# Map.centerObject(insitu2018_filtered, 11)\n",
        "# Map\n",
        "\n",
        "insitu2018 = ee.FeatureCollection('projects/ee-seagrass-mar/assets/Brazil/fieldData18').filterBounds(OSW)\n",
        "\n",
        "# RH_2018 = ee.FeatureCollection('projects/ee-seagrass-mar/assets/Brazil/field_RH_2018').filterBounds(OSW)\n",
        "\n",
        "# Filter for sand class and get size on the server side\n",
        "sand18 = insitu2018.filter(ee.Filter.eq('Bottom', 'sand'))\n",
        "# print('Total samples 2018:', insitu2018.size().getInfo())\n",
        "# print('Sand sample 2018:',sand18.size().getInfo())\n",
        "\n",
        "# in situ 2024 has 6 different bottom types: sand, mud, rock, algae, Halodule and Ruppia species, so we'll group them later ['Halodule, 'Ruppia'] == seagrass\n",
        "insitu2024 = ee.FeatureCollection('projects/ee-seagrass-mar/assets/Brazil/fieldData24').filterBounds(OSW)\n",
        "\n",
        "# Filter for sand class and get size on the server side\n",
        "sand24 = insitu2024.filter(ee.Filter.eq('Bottom', 'sand'))\n",
        "# seagrass24 = insitu2024.filter(ee.Filter.inList('Bottom', ['Halodule', 'Ruppia'])) # Keep this commented out as it was before\n",
        "# print('Total samples 2024:', insitu2024.size().getInfo())\n",
        "# print('Sand sample 2024:', sand24.size().getInfo())\n",
        "# print('Seagrass sample 2024:',seagrass24.size().getInfo()) # Keep this commented out as it was before\n",
        "insitu2018"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOBNMNW0LtEC"
      },
      "outputs": [],
      "source": [
        "Map.addLayer(insitu2018, {'color': 'darkgreen'}, 'in situ 2018')\n",
        "Map.addLayer(insitu2024, {'color': 'lightgreen'}, 'in situ 2024')\n",
        "# Map.addLayer(land_mask, {'color': 'blue'}, 'secchi_mask')\n",
        "Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYHBHIRBF8SX"
      },
      "outputs": [],
      "source": [
        "# Extract spectral data for each band for the whole the collection\n",
        "def sample_image_pre(image):\n",
        "    sampled = image.sampleRegions(\n",
        "        collection=insitu2018,\n",
        "        properties=['Bottom'],\n",
        "        scale=3,\n",
        "        geometries=True\n",
        "    ).filter(ee.Filter.notNull(image.bandNames()))\n",
        "    return sampled\n",
        "\n",
        "# Map the sampling function to the ImageCollection\n",
        "sampled_fc_list = multi_sensor_pre.map(sample_image_pre).toList(multi_sensor_pre.size())\n",
        "\n",
        "# Flatten the FeatureCollections into one big FeatureCollection\n",
        "sampledData_2018 = ee.FeatureCollection(sampled_fc_list).flatten()\n",
        "\n",
        "\n",
        "# print('sampledData 2018:', sampledData_2018.size().getInfo())\n",
        "\n",
        "def sample_image_post(image):\n",
        "    sampled = image.sampleRegions(\n",
        "        collection=insitu2024,\n",
        "        properties=['Bottom'],\n",
        "        scale=3,\n",
        "        geometries=True\n",
        "    ).filter(ee.Filter.notNull(image.bandNames()))\n",
        "    return sampled\n",
        "\n",
        "# Map the sampling function to the ImageCollection\n",
        "sampled_fc_list = multi_sensor_post.map(sample_image_post).toList(multi_sensor_post.size())\n",
        "\n",
        "# Flatten the FeatureCollections into one big FeatureCollection\n",
        "sampledData_2024 = ee.FeatureCollection(sampled_fc_list).flatten()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfRU81XdaywU"
      },
      "outputs": [],
      "source": [
        "# ## Sample data for Ruppia/Halodule in 2018\n",
        "# def sample_image_post(image):\n",
        "#     sampled = image.sampleRegions(\n",
        "#         collection=RH_2018,\n",
        "#         properties=['FONDO'],\n",
        "#         scale=3,\n",
        "#         geometries=True\n",
        "#     ).filter(ee.Filter.notNull(image.bandNames()))\n",
        "#     return sampled\n",
        "\n",
        "# # Map the sampling function to the ImageCollection\n",
        "# sampled_fc_list = multi_sensor_pre.map(sample_image_post).toList(multi_sensor_pre.size())\n",
        "\n",
        "# # Flatten the FeatureCollections into one big FeatureCollection\n",
        "# sampledData_2018_RH = ee.FeatureCollection(sampled_fc_list).flatten()\n",
        "# sampledData_2018_RH\n",
        "# # sampledData_2018"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bNFATaAMXjO"
      },
      "source": [
        "Chi-square analysis 2018"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNu2bm3ELRKR"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "import pandas as pd\n",
        "import ee\n",
        "import numpy as np # Import numpy for inf/nan handling\n",
        "import matplotlib.pyplot as plt # Import for plotting\n",
        "import seaborn as sns # Import for plotting\n",
        "\n",
        "# Assume ee.Initialize() has been run\n",
        "# Assume sampledData_2018 is your ee.FeatureCollection\n",
        "\n",
        "# Convert the Earth Engine FeatureCollection to a list of dictionaries\n",
        "try:\n",
        "    data_2018_list = sampledData_2018.getInfo()['features']\n",
        "except Exception as e:\n",
        "    print(f\"Error getting info from FeatureCollection: {e}\")\n",
        "    print(\"Please ensure sampledData_2018 is properly initialized and contains data.\")\n",
        "    data_2018_list = [] # Initialize empty list if there's an error\n",
        "\n",
        "# Convert the list of dictionaries to a pandas DataFrame\n",
        "if data_2018_list:\n",
        "    # Extract the properties dictionary from each feature\n",
        "    properties_list = [feature['properties'] for feature in data_2018_list]\n",
        "    df_2018 = pd.DataFrame(properties_list)\n",
        "\n",
        "    # Map 'seagrass' to 0 and 'sand' to 1 for the target variable\n",
        "    df_2018['habitat'] = df_2018['Bottom'].map({'seagrass': 0, 'sand': 1})\n",
        "\n",
        "    # Drop the original 'Bottom' column and any other irrelevant columns (like 'system:index' or geometry columns if present)\n",
        "    cols_to_drop = ['Bottom', 'system:index'] # Add any other columns you don't want as features\n",
        "    df_2018 = df_2018.drop(columns=[col for col in cols_to_drop if col in df_2018.columns])\n",
        "\n",
        "    # --- Remove the specific variables by name ---\n",
        "    variables_to_remove = ['PS_Kd_505', 'PS_KPAR', 'S2_SPM_Nechad2016_665', 'S2_chl_re_mishra']\n",
        "    # Drop columns if they exist in the DataFrame\n",
        "    df_2018 = df_2018.drop(columns=[col for col in variables_to_remove if col in df_2018.columns])\n",
        "    # --------------------------------------------\n",
        "\n",
        "\n",
        "    # Separate features (X) and target variable (y)\n",
        "    # Ensure 'habitat' is the target and all other columns are features\n",
        "    if 'habitat' in df_2018.columns:\n",
        "        X = df_2018.drop('habitat', axis=1)\n",
        "        y = df_2018['habitat']\n",
        "    else:\n",
        "        print(\"Error: 'habitat' column not found after removing variables.\")\n",
        "        X = pd.DataFrame() # Empty DataFrame if 'habitat' is missing\n",
        "        y = pd.Series(dtype=float) # Empty Series if 'habitat' is missing\n",
        "\n",
        "\n",
        "    # Chi-square works with non-negative values.\n",
        "    # Spectral reflectance values can be negative after atmospheric correction or processing.\n",
        "    # You might need to transform your data to be non-negative.\n",
        "    # A common approach is to scale the data to a positive range or use absolute values.\n",
        "    # For demonstration, we'll use absolute values. Consider if this is appropriate for your data.\n",
        "    X = X.abs() # Example transformation\n",
        "\n",
        "    # Handle potential NaN or infinite values that might arise from transformations or data issues\n",
        "    X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "    y = y[X.index] # Ensure y aligns with the filtered X\n",
        "\n",
        "    # Proceed with feature selection only if X is not empty\n",
        "    if not X.empty:\n",
        "        # Create a SelectKBest instance with chi2 as the scoring function\n",
        "        k_best_features = SelectKBest(score_func=chi2, k='all') # Use k='all' to see scores for all features\n",
        "\n",
        "        # Fit the SelectKBest instance to your data\n",
        "        k_best_features.fit(X, y)\n",
        "\n",
        "        # Get the scores for each feature\n",
        "        feature_scores = pd.DataFrame({'Feature': X.columns, 'Chi2_Score': k_best_features.scores_})\n",
        "\n",
        "        # Sort the features by their Chi-square scores in descending order\n",
        "        feature_scores = feature_scores.sort_values(by='Chi2_Score', ascending=False)\n",
        "\n",
        "        print(\"Chi-square scores for features (2018 data) after removing specified variables:\")\n",
        "        print(feature_scores)\n",
        "\n",
        "        # Plot results\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x='Chi2_Score', y='Feature', data=feature_scores, palette='viridis')\n",
        "        plt.title('Chi-square Scores for bands (2018 data)')\n",
        "        plt.xlabel('Chi-square Score')\n",
        "        plt.ylabel('Feature')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Cannot perform feature selection. Feature DataFrame is empty.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Could not process sampledData_2018. DataFrame is empty.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMX8mQcGNNMc"
      },
      "source": [
        "Chi-square analysis 2024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHqJg9mmMkrO"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "import pandas as pd\n",
        "import ee\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assume ee.Initialize() has been run\n",
        "# Assume sampledData_2024 is your ee.FeatureCollection\n",
        "\n",
        "# Convert the Earth Engine FeatureCollection to a list of dictionaries\n",
        "try:\n",
        "    data_2024_list = sampledData_2024.getInfo()['features']\n",
        "except Exception as e:\n",
        "    print(f\"Error getting info from FeatureCollection: {e}\")\n",
        "    print(\"Please ensure sampledData_2024 is properly initialized and contains data.\")\n",
        "    data_2024_list = []\n",
        "\n",
        "# Convert the list of dictionaries to a pandas DataFrame\n",
        "if data_2024_list:\n",
        "    # Extract the properties dictionary from each feature\n",
        "    properties_list = [feature['properties'] for feature in data_2024_list]\n",
        "    df_2024 = pd.DataFrame(properties_list)\n",
        "\n",
        "    # Map 'Halodule' and 'Ruppia' to 0 (seagrass) and 'sand' and 'mud' to 1 (non-seagrass)\n",
        "    # Handle other values if necessary, perhaps map them to NaN and drop later\n",
        "    df_2024['habitat'] = df_2024['Bottom'].map({'Halodule': 0, 'Ruppia': 0, 'sand': 1, 'mud': 1})\n",
        "\n",
        "    # Drop the original 'Bottom' column and any other irrelevant columns (like 'system:index' or geometry columns if present)\n",
        "    cols_to_drop = ['Bottom', 'system:index'] # Add any other columns you don't want as features\n",
        "    df_2024 = df_2024.drop(columns=[col for col in cols_to_drop if col in df_2024.columns])\n",
        "\n",
        "    # Remove rows where 'habitat' is NaN (for 'rock' or 'algae' if not included in mapping)\n",
        "    df_2024 = df_2024.dropna(subset=['habitat'])\n",
        "\n",
        "    # --- Remove the specific variables by name ---\n",
        "    variables_to_remove = ['PS_Kd_492', 'PS_KPAR', 'S2_SPM_Nechad2016_665', 'S2_chl_re_mishra']\n",
        "    # Drop columns if they exist in the DataFrame\n",
        "    df_2024 = df_2024.drop(columns=[col for col in variables_to_remove if col in df_2024.columns])\n",
        "    # --------------------------------------------\n",
        "\n",
        "    # Separate features (X) and target variable (y)\n",
        "    if 'habitat' in df_2024.columns:\n",
        "        X = df_2024.drop('habitat', axis=1)\n",
        "        y = df_2024['habitat'].astype(int) # Ensure y is integer type for chi2\n",
        "    else:\n",
        "        print(\"Error: 'habitat' column not found after removing variables.\")\n",
        "        X = pd.DataFrame()\n",
        "        y = pd.Series(dtype=int)\n",
        "\n",
        "    # Chi-square works with non-negative values.\n",
        "    X = X.abs()\n",
        "\n",
        "    # Handle potential NaN or infinite values\n",
        "    X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "    y = y[X.index]\n",
        "\n",
        "    # Proceed with feature selection only if X is not empty\n",
        "    if not X.empty and not y.empty:\n",
        "        # Create a SelectKBest instance\n",
        "        k_best_features = SelectKBest(score_func=chi2, k='all')\n",
        "\n",
        "        # Fit the SelectKBest instance to your data\n",
        "        k_best_features.fit(X, y)\n",
        "\n",
        "        # Get the scores for each feature\n",
        "        feature_scores = pd.DataFrame({'Feature': X.columns, 'Chi2_Score': k_best_features.scores_})\n",
        "\n",
        "        # Sort the features by their Chi-square scores\n",
        "        feature_scores = feature_scores.sort_values(by='Chi2_Score', ascending=False)\n",
        "\n",
        "        print(\"Chi-square scores for features (2024 data) after applying specified grouping:\")\n",
        "        print(feature_scores)\n",
        "\n",
        "        # Plot results\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x='Chi2_Score', y='Feature', data=feature_scores, palette='viridis')\n",
        "        plt.title('Chi-square Scores for bands (2024 data)')\n",
        "        plt.xlabel('Chi-square Score')\n",
        "        plt.ylabel('Feature')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Cannot perform feature selection. Feature or target DataFrame is empty after cleaning.\")\n",
        "\n",
        "else:\n",
        "    print(\"Could not process sampledData_2024. DataFrame is empty.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sK4v5OFEhr2m"
      },
      "outputs": [],
      "source": [
        "# ## Attach Rrs values to obtain spectral signatures for each sensor\n",
        "# dict_S2 = {\n",
        "#     'S2_B1': '443',\n",
        "#     'S2_B2': '492',\n",
        "#     'S2_B3': '560',\n",
        "#     'S2_B4': '666',\n",
        "#     'S2_B5': '704',\n",
        "#     'S2_B6': '740',\n",
        "#     'S2_B7': '783',\n",
        "#     'S2_B8': '834',\n",
        "#     'S2_B8A': '866',\n",
        "#     'S2_B9': '946',\n",
        "#     'S2_B10': '1375',\n",
        "#     'S2_B11': '1610',\n",
        "#     'S2_B12': '2200'\n",
        "# }\n",
        "\n",
        "# dict_PS2 = {\n",
        "#     'PS_B1': '485',\n",
        "#     'PS_B2': '566',\n",
        "#     'PS_B3': '665',\n",
        "#     'PS_B4': '865',\n",
        "# }\n",
        "\n",
        "# dict_PSB_SD = {\n",
        "#     'PS_B1': '444',\n",
        "#     'PS_B2': '490',\n",
        "#     'PS_B3': '531',\n",
        "#     'PS_B4': '565',\n",
        "#     'PS_B5': '610',\n",
        "#     'PS_B6': '665',\n",
        "#     'PS_B7': '705',\n",
        "#     'PS_B8': '865',\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8r6gTF-jPGt"
      },
      "outputs": [],
      "source": [
        "# def sum_dicts(dict1, dict2):\n",
        "#   result = dict1.copy()\n",
        "\n",
        "#   for key, value in dict2.items():\n",
        "#     if key in result:\n",
        "#       # Use string concatenation to 'add' values if keys match\n",
        "#       # result[key] += value  # Previous incorrect line\n",
        "#       result[key] = str(result[key]) + str(value)\n",
        "#     else:\n",
        "#       result[key] = value  # Otherwise, add the key-value pair\n",
        "\n",
        "#   return result\n",
        "\n",
        "# dict_18 = sum_dicts(dict_S2, dict_PS2)\n",
        "# dict_24 = sum_dicts(dict_S2, dict_PSB_SD)\n",
        "# print(dict_24)\n",
        "\n",
        "# # Remove keys from dict_18 that are not in sampledData_2018\n",
        "# # Get the property names from the first feature in sampledData_2024\n",
        "# property_names = sampledData_2018.first().propertyNames().getInfo()\n",
        "\n",
        "# # Filter dict_24 to keep only keys present in property_names\n",
        "# dict_18_filtered = {key: value for key, value in dict_18.items() if key in property_names}\n",
        "# print(dict_18_filtered)\n",
        "\n",
        "# # Function to rename properties of a feature\n",
        "# def rename_properties(feature):\n",
        "#   # Get the original properties\n",
        "#   properties = feature.toDictionary()\n",
        "\n",
        "#   # Convert dict_24_filtered to an ee.Dictionary\n",
        "#   ee_dict_18 = ee.Dictionary(dict_18_filtered)\n",
        "\n",
        "#   # Get the keys (original names) and values (new names) from dict_24_filtered\n",
        "#   from_names = list(dict_18_filtered.keys())\n",
        "#   to_names = list(dict_18_filtered.values())\n",
        "\n",
        "#   # Rename properties using from_names and to_names\n",
        "#   renamed_properties = properties.rename(from_names, to_names)\n",
        "\n",
        "#   # Return the feature with renamed properties\n",
        "#   return feature.set(renamed_properties)\n",
        "\n",
        "# # Apply the renaming function to the FeatureCollection\n",
        "# renamed_sampledData_2018 = sampledData_2018.map(rename_properties)\n",
        "# # renamed_sampledData_2024.first().propertyNames().getInfo()\n",
        "# renamed_sampledData_2018\n",
        "\n",
        "\n",
        "\n",
        "# # Remove keys from dict_24 that are not in sampledData_2024\n",
        "# # Get the property names from the first feature in sampledData_2024\n",
        "# property_names = sampledData_2024.first().propertyNames().getInfo()\n",
        "\n",
        "# # Filter dict_24 to keep only keys present in property_names\n",
        "# dict_24_filtered = {key: value for key, value in dict_24.items() if key in property_names}\n",
        "# print(dict_24_filtered)\n",
        "\n",
        "# # Function to rename properties of a feature\n",
        "# def rename_properties(feature):\n",
        "#   # Get the original properties\n",
        "#   properties = feature.toDictionary()\n",
        "\n",
        "#   # Convert dict_24_filtered to an ee.Dictionary\n",
        "#   ee_dict_24 = ee.Dictionary(dict_24_filtered)\n",
        "\n",
        "#   # Get the keys (original names) and values (new names) from dict_24_filtered\n",
        "#   from_names = list(dict_24_filtered.keys())\n",
        "#   to_names = list(dict_24_filtered.values())\n",
        "\n",
        "#   # Rename properties using from_names and to_names\n",
        "#   renamed_properties = properties.rename(from_names, to_names)\n",
        "\n",
        "#   # Return the feature with renamed properties\n",
        "#   return feature.set(renamed_properties)\n",
        "\n",
        "# # Apply the renaming function to the FeatureCollection\n",
        "# renamed_sampledData_2024 = sampledData_2024.map(rename_properties)\n",
        "# # renamed_sampledData_2024.first().propertyNames().getInfo()\n",
        "# # renamed_sampledData_2024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aohhHo4fJ83_"
      },
      "outputs": [],
      "source": [
        "# # Export the FeatureCollection to Google Drive\n",
        "# task = ee.batch.Export.table.toDrive(\n",
        "#     collection=sampledData_2018_RH,\n",
        "#     description='Signature_sampledData_2018_RH_OSW',  # Description for the task\n",
        "#     folder='Colab_Notebooks/export',  # Replace with your desired folder in Google Drive\n",
        "#     fileFormat='CSV'  # Choose your desired file format (CSV, GeoJSON, KML, SHP, TFRecord)\n",
        "# )\n",
        "\n",
        "# # Start the export task\n",
        "# task.start()\n",
        "\n",
        "# print('Export task started. You can monitor its progress in the \"Tasks\" tab.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s11Zn0bpf_Ht"
      },
      "outputs": [],
      "source": [
        "## Split training and validation sets ## Be sure in a Random Forest that the dataset is balanced (50%/50% of each class)\n",
        "## 2018\n",
        "filtered_18_seagrass = insitu2018.filter(ee.Filter.eq('Bottom', 'seagrass')).randomColumn('random')\n",
        "filtered_18_nonSeagrass = insitu2018.filter(ee.Filter.eq('Bottom', 'sand')).randomColumn('random')\n",
        "\n",
        "# Get the count of seagrass and sand points in filtered_18\n",
        "seagrass_samples_18 = filtered_18_seagrass.limit(90)\n",
        "nonSeagrass_samples_18 = filtered_18_nonSeagrass.limit(90)\n",
        "\n",
        "seagrass_count_18 = seagrass_samples_18.size().getInfo()\n",
        "nonSeagrass_count_18 = nonSeagrass_samples_18.size().getInfo()\n",
        "\n",
        "# Create labels and counts for the bar chart\n",
        "labels = ['Seagrass', 'Non-Seagrass']\n",
        "counts = [seagrass_count_18, nonSeagrass_count_18]\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(labels, counts, color=['green', 'darkorange'])\n",
        "plt.ylabel('Number of Points')\n",
        "plt.title('Distribution of Seagrass vs. Sand Points in filtered_18')\n",
        "plt.show()\n",
        "\n",
        "## 2024\n",
        "filtered_24_seagrass = insitu2024.filter(ee.Filter.inList('Bottom', ['Hadolule', 'Ruppia'])).randomColumn('random')\n",
        "filtered_24_nonSeagrass = insitu2024.filter(ee.Filter.inList('Bottom', ['sand',  'mud'])).randomColumn('random')\n",
        "\n",
        "## 2024\n",
        "# Separate features by class groupings\n",
        "seagrass_samples_24 = filtered_24_seagrass.filter(ee.Filter.inList('Bottom', ['Hadolule', 'Ruppia'])).limit(60)\n",
        "nonSeagrass_samples_24 = filtered_24_nonSeagrass.filter(ee.Filter.inList('Bottom', ['sand',  'mud', 'rock', 'algae'])).limit(60) # Adjust this list based on your non-seagrass classes\n",
        "\n",
        "# Get the count of seagrass and non-seagrass points in filtered_24\n",
        "seagrass_count_24 = seagrass_samples_24.size().getInfo()\n",
        "nonSeagrass_count_24 = nonSeagrass_samples_24.size().getInfo()\n",
        "\n",
        "# Create labels and counts for the bar chart\n",
        "labels = ['Seagrass', 'Non-Seagrass']\n",
        "counts = [seagrass_count_24, nonSeagrass_count_24]\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(labels, counts, color=['green', 'darkorange'])\n",
        "plt.ylabel('Number of Points')\n",
        "plt.title('Distribution of Seagrass vs. Non-Seaglass Points in filtered_24')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2n4tXCzj7c5u"
      },
      "outputs": [],
      "source": [
        "## Split training and validation sets\n",
        "## 2018\n",
        "t_seagrass_pre = seagrass_samples_18.filter(ee.Filter.eq('Bottom', 'seagrass')).filter(ee.Filter.lt('random', 0.7))\n",
        "v_seagrass_pre = seagrass_samples_18.filter(ee.Filter.eq('Bottom', 'seagrass')).filter(ee.Filter.gte('random', 0.7))\n",
        "t_nonSeagrass_pre = nonSeagrass_samples_18.filter(ee.Filter.eq('Bottom', 'sand')).filter(ee.Filter.lt('random', 0.7))\n",
        "v_nonSeagrass_pre = nonSeagrass_samples_18.filter(ee.Filter.eq('Bottom', 'sand')).filter(ee.Filter.gte('random', 0.7))\n",
        "validation_pre = v_seagrass_pre.merge(v_nonSeagrass_pre)\n",
        "print('Training pre:', t_seagrass_pre.merge(t_nonSeagrass_pre).size().getInfo())\n",
        "print('t_seagrass_pre:', t_seagrass_pre.size().getInfo())\n",
        "print('t_nonSeagrass_pre:', t_nonSeagrass_pre.size().getInfo())\n",
        "print('Validation pre:', validation_pre.size().getInfo())\n",
        "print('v_seagrass_pre:', v_seagrass_pre.size().getInfo())\n",
        "print('v_nonSeagrass_pre:', v_nonSeagrass_pre.size().getInfo())\n",
        "\n",
        "## 2024\n",
        "t_seagrass_post = seagrass_samples_24.filter(ee.Filter.inList('Bottom', ['Hadolule', 'Ruppia'])).filter(ee.Filter.lt('random', 0.7))\n",
        "v_seagrass_post = seagrass_samples_24.filter(ee.Filter.inList('Bottom', ['Hadolule', 'Ruppia'])).filter(ee.Filter.gte('random', 0.7))\n",
        "t_nonSeagrass_post = nonSeagrass_samples_24.filter(ee.Filter.inList('Bottom', ['sand',  'mud'])).filter(ee.Filter.lt('random', 0.7)) ##['sand', 'rock', 'mud', 'algae']))\n",
        "v_nonSeagrass_post = nonSeagrass_samples_24.filter(ee.Filter.inList('Bottom', ['sand', 'mud'])).filter(ee.Filter.gte('random', 0.7)) ## Try including only sand and mud\n",
        "validation_post = v_seagrass_post.merge(v_nonSeagrass_post)\n",
        "print('Training post:', t_seagrass_post.merge(t_nonSeagrass_post).size().getInfo())\n",
        "print('t_seagrass_post:', t_seagrass_post.size().getInfo())\n",
        "print('t_nonSeagrass_post:', t_nonSeagrass_post.size().getInfo())\n",
        "print('Validation post:', v_seagrass_post.merge(v_nonSeagrass_post).size().getInfo())\n",
        "print('v_seagrass_post:', v_seagrass_post.size().getInfo())\n",
        "print('v_nonSeagrass_post:', v_nonSeagrass_post.size().getInfo())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4w_iE7uAfgY"
      },
      "outputs": [],
      "source": [
        "multi_sensor_post"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkUkRoVY6Nae"
      },
      "source": [
        "# 5 - Depth Invariant Index (DII) - band ratios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGDBSxD5jODb"
      },
      "outputs": [],
      "source": [
        "def S2_depthInvariant_pre(image):\n",
        "  band1 = ['S2_B1', 'S2_B2', 'S2_B1']\n",
        "  band2 = ['S2_B2', 'S2_B3', 'S2_B3']\n",
        "  nband = ['S2_B1B2', 'S2_B2B3', 'S2_DII']\n",
        "\n",
        "  for i in range(3):\n",
        "    x = band1[i]\n",
        "    y = band2[i]\n",
        "    z = nband[i]\n",
        "\n",
        "    imageLog = image.select([x, y]).log()\n",
        "\n",
        "    sand = insitu2018.filter(ee.Filter.eq('Bottom', 'sand'))\n",
        "\n",
        "    covariance = imageLog.toArray().reduceRegion(\n",
        "        reducer=ee.Reducer.covariance(),\n",
        "        geometry=sand,\n",
        "        scale=3,\n",
        "        maxPixels=1e13,\n",
        "        bestEffort=True\n",
        "    )\n",
        "\n",
        "    covarMatrix = ee.Array(covariance.get('array'))\n",
        "    var1 = covarMatrix.get([0, 0])\n",
        "    var2 = covarMatrix.get([1, 1])\n",
        "    covar = covarMatrix.get([0, 1])\n",
        "\n",
        "    a = (var1.subtract(var2)).divide(covar.multiply(2))\n",
        "    attenCoeffRatio = a.add(((a.pow(2)).add(1)).sqrt())\n",
        "\n",
        "    depthInvariantIndex = image.expression(\n",
        "        'image1 - (image2 * coeff)', {\n",
        "            'image1': imageLog.select([x]),\n",
        "            'image2': imageLog.select([y]),\n",
        "            'coeff': attenCoeffRatio\n",
        "        }\n",
        "    )\n",
        "\n",
        "    image = image.addBands(depthInvariantIndex.select([x], [z]))\n",
        "\n",
        "  return image\n",
        "\n",
        "def S2_depthInvariant_post(image):\n",
        "  band1 = ['S2_B1', 'S2_B2', 'S2_B1']\n",
        "  band2 = ['S2_B2', 'S2_B3', 'S2_B3']\n",
        "  nband = ['S2_B1B2', 'S2_B2B3', 'S2_DII']\n",
        "\n",
        "  for i in range(3):\n",
        "    x = band1[i]\n",
        "    y = band2[i]\n",
        "    z = nband[i]\n",
        "\n",
        "    imageLog = image.select([x, y]).log()\n",
        "\n",
        "    sand = insitu2024.filter(ee.Filter.eq('Bottom', 'sand'))\n",
        "\n",
        "    covariance = imageLog.toArray().reduceRegion(\n",
        "        reducer=ee.Reducer.covariance(),\n",
        "        geometry=sand,\n",
        "        scale=3,\n",
        "        maxPixels=1e13,\n",
        "        bestEffort=True\n",
        "    )\n",
        "\n",
        "    covarMatrix = ee.Array(covariance.get('array'))\n",
        "    var1 = covarMatrix.get([0, 0])\n",
        "    var2 = covarMatrix.get([1, 1])\n",
        "    covar = covarMatrix.get([0, 1])\n",
        "\n",
        "    a = (var1.subtract(var2)).divide(covar.multiply(2))\n",
        "    attenCoeffRatio = a.add(((a.pow(2)).add(1)).sqrt())\n",
        "\n",
        "    depthInvariantIndex = image.expression(\n",
        "        'image1 - (image2 * coeff)', {\n",
        "            'image1': imageLog.select([x]),\n",
        "            'image2': imageLog.select([y]),\n",
        "            'coeff': attenCoeffRatio\n",
        "        }\n",
        "    )\n",
        "\n",
        "    image = image.addBands(depthInvariantIndex.select([x], [z]))\n",
        "\n",
        "  return image\n",
        "\n",
        "# For PlanetScope Classic\n",
        "def PSC_depthInvariant(image):\n",
        "  band1 = ['PS_B1', 'PS_B2', 'PS_B1']\n",
        "  band2 = ['PS_B2', 'PS_B3', 'PS_B2']\n",
        "  nband = ['PS_B1B2', 'PS_B2B3', 'PS_DII']\n",
        "\n",
        "  for i in range(3):\n",
        "    x = band1[i]\n",
        "    y = band2[i]\n",
        "    z = nband[i]\n",
        "\n",
        "    imageLog = image.select([x, y]).log()\n",
        "\n",
        "    sand = insitu2018.filter(ee.Filter.eq('Bottom', 'sand'))\n",
        "\n",
        "    covariance = imageLog.toArray().reduceRegion(\n",
        "        reducer=ee.Reducer.covariance(),\n",
        "        geometry=sand,\n",
        "        scale=3,\n",
        "        maxPixels=1e13,\n",
        "        bestEffort=True\n",
        "    )\n",
        "\n",
        "    covarMatrix = ee.Array(covariance.get('array'))\n",
        "    var1 = covarMatrix.get([0, 0])\n",
        "    var2 = covarMatrix.get([1, 1])\n",
        "    covar = covarMatrix.get([0, 1])\n",
        "\n",
        "    a = (var1.subtract(var2)).divide(covar.multiply(2))\n",
        "    attenCoeffRatio = a.add(((a.pow(2)).add(1)).sqrt())\n",
        "\n",
        "    depthInvariantIndex = image.expression(\n",
        "        'image1 - (image2 * coeff)', {\n",
        "            'image1': imageLog.select([x]),\n",
        "            'image2': imageLog.select([y]),\n",
        "            'coeff': attenCoeffRatio\n",
        "        }\n",
        "    )\n",
        "\n",
        "    image = image.addBands(depthInvariantIndex.select([x], [z]))\n",
        "\n",
        "  return image\n",
        "\n",
        "# For PlanetScope SuperDove\n",
        "def PSSD_depthInvariant(image):\n",
        "  band1 = ['PS_B2', 'PS_B3', 'PS_B2']\n",
        "  band2 = ['PS_B3', 'PS_B4', 'PS_B4']\n",
        "  nband = ['PS_B2B3', 'PS_B3B4', 'PS_DII']\n",
        "\n",
        "  for i in range(3):\n",
        "    x = band1[i]\n",
        "    y = band2[i]\n",
        "    z = nband[i]\n",
        "\n",
        "    imageLog = image.select([x, y]).log()\n",
        "\n",
        "    sand = insitu2024.filter(ee.Filter.eq('Bottom', 'sand'))\n",
        "\n",
        "    covariance = imageLog.toArray().reduceRegion(\n",
        "        reducer=ee.Reducer.covariance(),\n",
        "        geometry=sand,\n",
        "        scale=3,\n",
        "        maxPixels=1e13,\n",
        "        bestEffort=True\n",
        "    )\n",
        "\n",
        "    covarMatrix = ee.Array(covariance.get('array'))\n",
        "    var1 = covarMatrix.get([0, 0])\n",
        "    var2 = covarMatrix.get([1, 1])\n",
        "    covar = covarMatrix.get([0, 1])\n",
        "\n",
        "    a = (var1.subtract(var2)).divide(covar.multiply(2))\n",
        "    attenCoeffRatio = a.add(((a.pow(2)).add(1)).sqrt())\n",
        "\n",
        "    depthInvariantIndex = image.expression(\n",
        "        'image1 - (image2 * coeff)', {\n",
        "            'image1': imageLog.select([x]),\n",
        "            'image2': imageLog.select([y]),\n",
        "            'coeff': attenCoeffRatio\n",
        "        }\n",
        "    )\n",
        "\n",
        "    image = image.addBands(depthInvariantIndex.select([x], [z]))\n",
        "\n",
        "  return image\n",
        "\n",
        "# Apply to your image collection (e.g., multi_sensor_pre or multi_sensor_post)\n",
        "multi_sensor_pre = multi_sensor_pre.map(S2_depthInvariant_pre).map(PSC_depthInvariant)\n",
        "multi_sensor_post = multi_sensor_post.map(S2_depthInvariant_post).map(PSSD_depthInvariant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOFh28f4h2pd"
      },
      "outputs": [],
      "source": [
        "# ## NDAVI index\n",
        "# # Function to calculate NDAVI and add it as a band\n",
        "# def S2_ndavi(image):\n",
        "#     ndavi = image.normalizedDifference(['S2_B8', 'S2_B2']).rename('S2_NDAVI')\n",
        "#     return image.addBands(ndavi)\n",
        "\n",
        "# def PSC_ndavi(image):\n",
        "#     ndavi = image.normalizedDifference(['PS_B4', 'PS_B1']).rename('PS_NDAVI')\n",
        "#     return image.addBands(ndavi)\n",
        "\n",
        "# def PSSD_ndavi(image):\n",
        "#     ndavi = image.normalizedDifference(['PS_B8', 'PS_B2']).rename('PS_NDAVI')\n",
        "#     return image.addBands(ndavi)\n",
        "\n",
        "# # Map function over collection\n",
        "# multi_sensor_pre = multi_sensor_pre.map(S2_ndavi).map(PSC_ndavi)\n",
        "# multi_sensor_post = multi_sensor_post.map(S2_ndavi).map(PSSD_ndavi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnHVgme88Ml8"
      },
      "source": [
        "# 6 - Binary classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDjKdPXu1N9f"
      },
      "source": [
        "## 6.1 - Pre-event Multi-sensor Image Collection\n",
        "#### Before processing all data, the best seagrass probability threshold was studied and set as t = 45"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAVChVMEfPJQ"
      },
      "outputs": [],
      "source": [
        "# Binary classification setup\n",
        "seagrass_class = 0\n",
        "nonSeagrass_class = 1\n",
        "\n",
        "# Classification and validation parameters\n",
        "trees = 50\n",
        "t = 45\n",
        "\n",
        "# Boxcar kernel: a square matrix where all values are equal, and it calculates the mean of the pixel values within that square neighborhood.\n",
        "boxcar = ee.Kernel.square(radius=2, units='pixels', normalize=True)\n",
        "bands = multi_sensor_pre.first().bandNames()\n",
        "\n",
        "def boxcar_image(image):\n",
        "    return image.convolve(boxcar)\n",
        "\n",
        "# Apply the function to every image in the collection\n",
        "classificationComp_pre = multi_sensor_pre.map(boxcar_image)\n",
        "classificationComp_pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mj4uP2Dy0l7"
      },
      "source": [
        " ##### 6.1.1 - Prepare training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8MPF_cC22dL"
      },
      "outputs": [],
      "source": [
        "# Training data - class property recoding and merging - MODEL 1 (PRE-EVENT)\n",
        "t_seagrass = t_seagrass_pre.map(lambda x: x.setMulti(ee.Dictionary.fromLists(['habitat'], [seagrass_class])))\n",
        "t_nonSeagrass = t_nonSeagrass_pre.map(lambda x: x.setMulti(ee.Dictionary.fromLists(['habitat'], [nonSeagrass_class])))\n",
        "t_FC = t_seagrass.merge(t_nonSeagrass)\n",
        "\n",
        "\n",
        "# Collect sampled features from each image\n",
        "def sample_image(image):\n",
        "    sampled = image.sampleRegions(\n",
        "        collection=t_FC,\n",
        "        scale=3,\n",
        "        geometries=True\n",
        "    ).filter(ee.Filter.notNull(bands))\n",
        "\n",
        "    return sampled\n",
        "\n",
        "# Map the sampling function and convert result to a list of FeatureCollections\n",
        "sampled_fc_list = classificationComp_pre.map(sample_image).toList(classificationComp_pre.size())\n",
        "sampled_fc_list\n",
        "# Flatten the FeatureCollections into one big FeatureCollection\n",
        "sampledData = ee.FeatureCollection(sampled_fc_list).flatten()\n",
        "\n",
        "# Using sampleRegions we take values across the Image Collection. getting valid FeatureCollection when overlapping with pixels\n",
        "print('sampledData:', sampledData.size().getInfo())\n",
        "print('t_FC:', t_FC.size().getInfo())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIHaemWuAMMK"
      },
      "source": [
        "6.1.2 - Extract probabilities for each class and train classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4jUchzkVbRO"
      },
      "outputs": [],
      "source": [
        "## To extract feature importance and evaluate\n",
        "def soft_prob_subfn(image, num):\n",
        "       training = sampledData.map(lambda ft: ft.set(\n",
        "        'prob',\n",
        "        ee.Algorithms.If(ft.getNumber('habitat').eq(num), 1, 0)\n",
        "        ))\n",
        "\n",
        "       trained = ee.Classifier.smileRandomForest(numberOfTrees=trees) \\\n",
        "           .train(training, 'prob', image.bandNames()) \\\n",
        "           .setOutputMode('PROBABILITY')\n",
        "\n",
        "       dict_classifier = trained.explain()\n",
        "\n",
        "       # Create a Feature with the dictionary as a property\n",
        "       feature = ee.Feature(None, {'classifier_explanation': dict_classifier})\n",
        "\n",
        "       # Classify and convert to percentage scale\n",
        "       classified = image.classify(trained).multiply(100).toInt8()\n",
        "\n",
        "       # Return both the classified image and the Feature with explanation\n",
        "       return ee.Image(classified).set('classifier_explanation', feature)\n",
        "\n",
        "# Map the function to your ImageCollection\n",
        "seagrass_prob_IC = classificationComp_pre.map(lambda img: soft_prob_subfn(img, seagrass_class))\n",
        "nonSeagrass_prob_IC = classificationComp_pre.map(lambda img: soft_prob_subfn(img, nonSeagrass_class))\n",
        "\n",
        "\n",
        "### MEAN FEATURE IMPORTANCE\n",
        "# Get the first three images from the ImageCollection\n",
        "first_three_images = seagrass_prob_IC.toList(3)\n",
        "\n",
        "# Initialize an empty dictionary to store feature importances\n",
        "mean_importance = {}\n",
        "\n",
        "# Iterate through the first three images\n",
        "for i in range(3):\n",
        "  image = ee.Image(first_three_images.get(i))\n",
        "  classifier_explanation = image.get('classifier_explanation').getInfo()['properties']['classifier_explanation']\n",
        "  importance_dict = classifier_explanation.get('importance')\n",
        "\n",
        "  # Accumulate feature importances\n",
        "  for feature, importance in importance_dict.items():\n",
        "    mean_importance[feature] = mean_importance.get(feature, 1) + importance\n",
        "\n",
        "# Calculate the mean feature importance\n",
        "for feature in mean_importance:\n",
        "  mean_importance[feature] /= 3\n",
        "\n",
        "# Print the mean feature importance\n",
        "print(mean_importance)\n",
        "\n",
        "# Sort features by importance in descending order\n",
        "sorted_importance = dict(sorted(mean_importance.items(), key=lambda item: item[1], reverse=False))\n",
        "\n",
        "# Extract feature names and importance values\n",
        "feature_names = list(sorted_importance.keys())\n",
        "importance_values = list(sorted_importance.values())\n",
        "\n",
        "# Create a list of colors based on feature name prefix\n",
        "colors = ['#005b96' if name.startswith('PS_') else '#6497b1' for name in feature_names]\n",
        "\n",
        "# Create the horizontal bar graph (switched axes)\n",
        "plt.figure(figsize=(10, 6))  # Adjust figure size if needed\n",
        "plt.barh(feature_names, importance_values, color=colors) # Changed to barh for horizontal bars\n",
        "plt.ylabel(\"Features\") # Switched x and y labels\n",
        "plt.xlabel(\"Importance\") # Switched x and y labels\n",
        "plt.title(\"Feature Importance (2018)\")\n",
        "# plt.axvline(x=10.5, linestyle='dotted', color='grey')  # Add dotted vertical line\n",
        "plt.savefig('feature_importance_2018.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()  # Display the plot\n",
        "plt.tight_layout()\n",
        "\n",
        "selected_bands = [band for band, importance in mean_importance.items() if importance >= 0] ## Use only selected bands for the classification ## 10.5 before\n",
        "\n",
        "print(\"Selected bands:\", selected_bands)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCpwD9Ye5oQT"
      },
      "source": [
        "Analyse covariance between variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQP47h8xySgf"
      },
      "outputs": [],
      "source": [
        "## Co-variance analysis\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to extract band values from a FeatureCollection\n",
        "def extract_band_values(feature_collection, bands):\n",
        "    data = feature_collection.toList(feature_collection.size()).map(lambda feature: ee.Feature(feature).toDictionary().select(bands)).getInfo()\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "# Extract band values for selected bands\n",
        "band_values_df = extract_band_values(sampledData, selected_bands)\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = band_values_df.corr()\n",
        "\n",
        "# Create a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(correlation_matrix, bool), k=1)\n",
        "\n",
        "# Apply the mask to the filtered correlation matrix\n",
        "filtered_correlation_matrix = correlation_matrix.mask(mask)\n",
        "filtered_correlation_matrix = filtered_correlation_matrix[np.abs(correlation_matrix) > 0.80]\n",
        "\n",
        "# Get bands with at least one correlation above 0.9\n",
        "bands_to_keep = filtered_correlation_matrix.columns[filtered_correlation_matrix.any()]\n",
        "\n",
        "# Filter the original correlation matrix to keep only relevant bands\n",
        "filtered_correlation_matrix = correlation_matrix.loc[bands_to_keep, bands_to_keep]\n",
        "\n",
        "# Plot only the lower triangle of the correlation matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(filtered_correlation_matrix, mask=mask, annot=True, cmap='coolwarm',\n",
        "            fmt=\".2f\", linewidths=.5, linecolor='lightgrey', square=True,\n",
        "            cbar_kws={\"shrink\": .75})\n",
        "plt.title('Band Covariability Analysis')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Bands to keep:\", bands_to_keep.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyGjsp4AAXc2"
      },
      "source": [
        "6.1.3 - Generate probability layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TH4oE005EqW"
      },
      "outputs": [],
      "source": [
        "## Train classifier and display probability map for seagrasses\n",
        "# Define classes dictionary\n",
        "classes = {\n",
        "  'classes_values': [seagrass_class, nonSeagrass_class],\n",
        "  'classes_names': ['seagrass','nonSeagrass']\n",
        "}\n",
        "\n",
        "classes\n",
        "\n",
        "def classify_with_all_classes(image):\n",
        "    # Classify image for each class and rename the result accordingly\n",
        "    classified_images = ee.List(classes['classes_values']).map(\n",
        "        lambda class_id: soft_prob_subfn(image, class_id)\n",
        "    )\n",
        "\n",
        "    # Convert list of images into one multi-band image (each class as band)\n",
        "    classified_combined = ee.ImageCollection(classified_images).toBands()\n",
        "    classified_combined = classified_combined.rename(classes['classes_names'])\n",
        "\n",
        "    # Preserve metadata if needed\n",
        "    return classified_combined.copyProperties(image, image.propertyNames())\n",
        "\n",
        "# Generate probabilities in the ImageCollection\n",
        "probabilities_pre = classificationComp_pre.map(classify_with_all_classes)\n",
        "probabilities_pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSPL9732m1C_"
      },
      "source": [
        "Plot probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "itJ4emCxBk9o"
      },
      "outputs": [],
      "source": [
        "# Visualize probabilities\n",
        "first_prob = ee.Image(probabilities_pre.toList(probabilities_pre.size()).get(0))\n",
        "Map = geemap.Map()\n",
        "Map.addLayer(first_prob.select('seagrass'), {'min': 45, 'max': 100}, 'Seagrass Prob')\n",
        "Map.addLayer(first_prob.select('nonSeagrass'), {'min': 45, 'max': 100}, 'NonSeagrass Prob')\n",
        "Map.centerObject(probabilities_pre.first(), zoom=12)\n",
        "Map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNagrZ1oDbpo"
      },
      "source": [
        "6.1.4 - Threshold classes based on probabilities and classify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYY-Mvw2BQLB"
      },
      "outputs": [],
      "source": [
        "# Based on the generated probabilities, apply thresholding for seagrass and nonSeagrass on the whole Image Collection\n",
        "\n",
        "# Threshold function for seagrass\n",
        "def threshold_seagrass(image):\n",
        "    return image.select('seagrass').updateMask(image.select('seagrass').gt(t)) \\\n",
        "        .copyProperties(image, image.propertyNames())\n",
        "\n",
        "# Threshold function for non-seagrass\n",
        "def threshold_non_seagrass(image):\n",
        "    return image.select('nonSeagrass').updateMask(image.select('nonSeagrass').gt(t)) \\\n",
        "        .copyProperties(image, image.propertyNames())\n",
        "\n",
        "# Map the thresholding over the probabilities collection\n",
        "seagrass_soft = probabilities_pre.map(threshold_seagrass)\n",
        "nonSeagrass_soft = probabilities_pre.map(threshold_non_seagrass)\n",
        "nonSeagrass_soft\n",
        "\n",
        "# Function to combine seagrass and nonSeagrass into one image\n",
        "def add_bands(image):\n",
        "    matching_non_seagrass = nonSeagrass_soft.filter(\n",
        "        ee.Filter.eq('system:index', image.get('system:index'))\n",
        "    ).first()\n",
        "\n",
        "    # If no match is found, return image as-is\n",
        "    combined = ee.Image(image).addBands(ee.Image(matching_non_seagrass))\n",
        "    return combined.copyProperties(image, image.propertyNames())\n",
        "\n",
        "# Map the function to seagrass_soft collection to create the aggregated collection\n",
        "soft_map = seagrass_soft.map(add_bands)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vUUUWdTAiph"
      },
      "source": [
        "6.1.5 - Perform classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yh_mn6kSL2Ww"
      },
      "outputs": [],
      "source": [
        "def soft_to_hard(image):\n",
        "    seagrass = image.select('seagrass')\n",
        "    non_seagrass = image.select('nonSeagrass')\n",
        "    prob_aoi = ee.Image(probabilities_pre.first()).geometry()\n",
        "\n",
        "    # Get the corresponding images from seagrass_soft and nonSeagrass_soft using system:index\n",
        "    seagrass_soft_image = seagrass_soft.filter(ee.Filter.eq('system:index', image.get('system:index'))).first()\n",
        "    nonSeagrass_soft_image = nonSeagrass_soft.filter(ee.Filter.eq('system:index', image.get('system:index'))).first()\n",
        "\n",
        "    classified = (ee.Image.constant(2)\n",
        "        .where(seagrass.gte(t), ee.Number(0).add(1))  # temporary +1 to mask 0s\n",
        "        .where(non_seagrass.gte(t), ee.Number(1).add(1))\n",
        "        .selfMask()\n",
        "        .subtract(1)  # back to 0 and 1\n",
        "        .rename('classification')\n",
        "        .clip(prob_aoi))\n",
        "\n",
        "    return classified.copyProperties(image, image.propertyNames())\n",
        "\n",
        "# Apply to soft_map\n",
        "soft_map_th = soft_map.map(soft_to_hard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHtDniYmRMGr"
      },
      "outputs": [],
      "source": [
        "# ## Export\n",
        "# image_to_export = soft_map_th.first()\n",
        "# # Define the export task\n",
        "# task = ee.batch.Export.image.toDrive(\n",
        "#     image=image_to_export,\n",
        "#     description='soft_map_th_first',  # Description for the task\n",
        "#     folder='Colab_Notebooks/export',  # Folder in Google Drive to save the image\n",
        "#     fileNamePrefix='soft_map_th_first',  # Prefix for the file name\n",
        "#     region=roi,  # Region of interest\n",
        "#     scale=3,  # Scale in meters\n",
        "#     fileFormat='GeoTIFF',  # File format\n",
        "#     maxPixels=1e13  # Maximum number of pixels allowed (adjust if needed)\n",
        "# )\n",
        "# # Start the export task\n",
        "# task.start()\n",
        "# print('Export task started. You can monitor its progress in the \"Tasks\" tab.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELPzK_LjA_8u"
      },
      "source": [
        "6.1.6 - Combine the three classifications for visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fluP7YI_Jzhd"
      },
      "outputs": [],
      "source": [
        "# 1. Extract the 'classification' band from each image\n",
        "classification_images_pre = soft_map_th.select('classification')\n",
        "\n",
        "# 2. Calculate the mean value\n",
        "combined_classification_pre = classification_images_pre.mean().mask(land_mask).clip(OSW) #.selfMask()\n",
        "# print(combined_classification.getInfo())\n",
        "\n",
        "# Display the combined classification\n",
        "Map.addLayer(combined_classification_pre, {'min': 0, 'max': 1, 'palette': ['42762f', '7cb75a', 'e2e9ab', 'ffffff']}, 'Combined Classification')\n",
        "Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klDw5KXJv-WM"
      },
      "outputs": [],
      "source": [
        "# # Export the FeatureCollection to Google Drive\n",
        "# task = ee.batch.Export.image.toDrive(\n",
        "#     image=combined_classification_pre,\n",
        "#     description='combined_classification_pre',  # Description for the task\n",
        "#     folder='Colab_Notebooks/export',\n",
        "#     region = OSW.geometry(),\n",
        "#     scale=3,  # Adjust the scale as needed\n",
        "#     fileFormat='GeoTIFF'  # Choose your desired file format (CSV, GeoJSON, KML, SHP, TFRecord)\n",
        "# )\n",
        "\n",
        "# # Start the export task\n",
        "# task.start()\n",
        "\n",
        "# print('Export task started. You can monitor its progress in the \"Tasks\" tab.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHPmTzSsHiVb"
      },
      "outputs": [],
      "source": [
        "# Apply conditional classification for obtain the final binary map\n",
        "final_classification_pre = combined_classification_pre.expression(\n",
        "    'b(0) < 0.35 ? 0 : 1',  # Conditional expression\n",
        "    {'classification': combined_classification_pre}  # Band name mapping\n",
        ").rename('classification')  # Rename the band\n",
        "\n",
        "# Display the combined classification\n",
        "Map.addLayer(final_classification_pre.mask(land_mask).clip(OSW.geometry()), {'min': 0, 'max': 1, 'palette': ['42762f', 'ffffff']}, 'Combined Classification')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f64ZabF7Avtu"
      },
      "source": [
        "6.1.7 - Validation and accuracy assessment for each of the 3 classifications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkstm3sWWQkH"
      },
      "outputs": [],
      "source": [
        "# Validation function\n",
        "def valid_habitat(number):\n",
        "  def wrap(feature):\n",
        "    return feature.setMulti(ee.Dictionary.fromLists(['habitat'], [number]))\n",
        "  return wrap\n",
        "\n",
        "## Validation data\n",
        "v_seagrass = v_seagrass_pre.map(valid_habitat(seagrass_class))\n",
        "v_nonSeagrass = v_nonSeagrass_pre.map(valid_habitat(nonSeagrass_class))\n",
        "v_FC = v_seagrass.merge(v_nonSeagrass)\n",
        "v_FC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWObll4oURfn"
      },
      "outputs": [],
      "source": [
        "# Collect sampled features from each image\n",
        "def validation(image):\n",
        "    # Buffer the validation points by 10 meters (optional, keep if desired)\n",
        "   # buffered_v_FC = v_FC.map(lambda feature: feature.buffer(5))\n",
        "\n",
        "    sampled = image.sampleRegions(\n",
        "        collection=v_FC,\n",
        "        properties=['habitat'],\n",
        "        scale=3\n",
        "        )\n",
        "    return sampled\n",
        "\n",
        "\n",
        "# Map the sampling function and convert result to a list of FeatureCollections\n",
        "sampled_v_list = soft_map_th.map(validation).toList(soft_map_th.size())\n",
        "\n",
        "# Flatten the FeatureCollections into one big FeatureCollection\n",
        "sampledValidation_pre = ee.FeatureCollection(sampled_v_list).flatten() #.filter(ee.Filter.notNull(['habitat']))\n",
        "\n",
        "## To validate with the final combination of the 3 classifications (this will be better)\n",
        "# sampledValidation_pre = final_classification_pre.sampleRegions(\n",
        "#     collection=v_FC,\n",
        "#     scale=3,\n",
        "#     properties=['habitat'])\n",
        "\n",
        "# Using sampleRegions we take values across the Image Collection. getting valid FeatureCollection when overlapping with pixels\n",
        "print('sampled Validation:', sampledValidation_pre.size().getInfo())\n",
        "sampledValidation_pre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3bbR77kcVHf"
      },
      "outputs": [],
      "source": [
        "errorMatrix = sampledValidation_pre.errorMatrix('habitat', 'classification')\n",
        "print('Confusion Matrix:', errorMatrix.getInfo())\n",
        "\n",
        "# Get confusion matrix data as a list of lists\n",
        "confusion_matrix_data = errorMatrix.array().getInfo()\n",
        "\n",
        "# Convert the list to a NumPy array\n",
        "matrix_values = np.array(confusion_matrix_data)  # Convert to NumPy array\n",
        "\n",
        "# Define class labels\n",
        "class_names = ['seagrass', 'nonSeagrass']  # Adjust if your class names are different\n",
        "\n",
        "# Create ConfusionMatrixDisplay object\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=matrix_values, display_labels=class_names)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel(\"EO prediction\", fontsize = 14)  # Change to your desired text\n",
        "plt.ylabel(\"In situ data\", fontsize = 14)\n",
        "disp.ax_.tick_params(axis='both', which='major', labelsize=18)  # Increase axis tick label size  # Change to your desired text\n",
        "plt.show()\n",
        "\n",
        "# Print accuracy metrics\n",
        "overall_accuracy = errorMatrix.accuracy().getInfo()\n",
        "print('Overall Accuracy:', overall_accuracy)\n",
        "producers_accuracy = ee.Array(errorMatrix.producersAccuracy()).reshape([-1]).getInfo()\n",
        "print('PA:', producers_accuracy)\n",
        "users_accuracy = ee.Array(errorMatrix.consumersAccuracy()).reshape([-1]).getInfo()\n",
        "print('UA:', users_accuracy)\n",
        "\n",
        "# Calcuate F1-score\n",
        "# Extract the components from the confusion matrix\n",
        "truePositives = errorMatrix.array().get([0, 0])\n",
        "falsePositives = errorMatrix.array().get([1, 0])\n",
        "falseNegatives = errorMatrix.array().get([0, 1])\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = truePositives.divide(truePositives.add(falsePositives))\n",
        "recall = truePositives.divide(truePositives.add(falseNegatives))\n",
        "f1_score = ee.Number(2).multiply(precision.multiply(recall)).divide(precision.add(recall))\n",
        "\n",
        "print('F1-score:', f1_score.getInfo())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voe8tW0e9pzW"
      },
      "source": [
        "### Area quantification stats and +-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfHt_UQ49oKf"
      },
      "outputs": [],
      "source": [
        "# Mask seagrass pixels\n",
        "seagrass_pre = final_classification_pre.eq(0)\n",
        "\n",
        "# create a binary mask: 1 = seagrass, 0 = nonSeagrass\n",
        "seagrass_mask = seagrass_pre.rename('seagrass')  # change eq(0) if your seagrass label differs\n",
        "\n",
        "# count seagrass pixels (sum of 1s)\n",
        "pixel_count = seagrass_mask.reduceRegion(\n",
        "    reducer=ee.Reducer.sum(),\n",
        "    geometry=OSW.geometry(),\n",
        "    scale=3, # 3 m pixels\n",
        "    maxPixels=1e13\n",
        ").get('seagrass').getInfo()\n",
        "\n",
        "# convert to hectares: each pixel = 9 m2 -> 9/10000 = 0.0009 ha\n",
        "area_ha_pre = pixel_count * 9.0 / 10000.0\n",
        "\n",
        "print(f\"Seagrass pixels: {pixel_count}, area: {area_ha_pre} ha\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94db260e"
      },
      "outputs": [],
      "source": [
        "# Scale probabilities from 0-100 to 0-1\n",
        "probabilities_pre = probabilities_pre.map(lambda image: image.divide(100))\n",
        "\n",
        "# Sum the seagrass probabilities for pre-event images\n",
        "sum_seagrass_probabilities_pre = probabilities_pre.select('seagrass').sum()\n",
        "sum_seagrass_probabilities_pre\n",
        "\n",
        "# Calculate the mean of the summed probabilities within the seagrass mask\n",
        "mean_sum_prob_seagrass_pre = sum_seagrass_probabilities_pre.updateMask(seagrass_mask).reduceRegion(\n",
        "    reducer=ee.Reducer.mean(),\n",
        "    geometry=OSW.geometry(),\n",
        "    scale=3, # Use the same scale as classification\n",
        "    maxPixels=1e13\n",
        ").get('seagrass')\n",
        "\n",
        "mean_sum_prob_seagrass_pre = ee.Number(mean_sum_prob_seagrass_pre)\n",
        "\n",
        "overall_accuracy_pre = ee.Number(overall_accuracy) # Use the overall_accuracy from the pre-event validation cell\n",
        "\n",
        "total_seagrass_area_pre = ee.Number(area_ha_pre)\n",
        "\n",
        "\n",
        "# Calculate uncertainty for the pre-event seagrass area using the provided formula:\n",
        "# uncertainty = area * Overall Accuracy * (3 - mean of the sum of the probabilities predicted as seagrass) / 3\n",
        "# Assuming '3' is the number of images used in the pre-event period (sentinel2_l1c_pre has 3 images)\n",
        "num_images_pre = ee.Number(sentinel2_l1c_pre.size()) # Get the actual number of pre-event images\n",
        "\n",
        "uncertainty_pre_seagrass_area = total_seagrass_area_pre.multiply(overall_accuracy_pre).multiply(num_images_pre.subtract(mean_sum_prob_seagrass_pre)).divide(num_images_pre)\n",
        "\n",
        "print(f\"Uncertainty of pre-event seagrass area: {area_ha_pre} +- {uncertainty_pre_seagrass_area.getInfo():.2f} ha\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0569l3h78ia"
      },
      "source": [
        "6.1.8 - Uncertainty analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-rgS_YS38TS"
      },
      "outputs": [],
      "source": [
        "# ## To run uncertainties in the first classification (pre-event, 2018)\n",
        "# ## t=(9*OA*(3-median probabilities))/3\n",
        "# # 9 = area of pixel in square meters (3x3)\n",
        "# # 3 = number of images\n",
        "\n",
        "# ## first scale probabilities to 0-1 values\n",
        "# def scale_to_unit(image):\n",
        "#     return image.divide(100).copyProperties(image, image.propertyNames())\n",
        "\n",
        "# # Apply the scaling function to the collection\n",
        "# probabilities_01 = probabilities.map(scale_to_unit)\n",
        "\n",
        "# median_probability = probabilities_01.select('seagrass').median().rename('seagrass_median_prob')\n",
        "# median_probability\n",
        "\n",
        "# # Select the band containing median probability\n",
        "# median_probability = median_probability.select('seagrass_median_prob')\n",
        "\n",
        "# # Apply the formula:\n",
        "# # uncertainty = (9 * overall_accuracy * (3 - median_probability)) / 3\n",
        "# three = ee.Number(3)\n",
        "# nine = ee.Number(9)\n",
        "# oa = ee.Number(overall_accuracy)\n",
        "\n",
        "# # Compute: (3 - median_prob)\n",
        "# diff = ee.Image.constant().subtract(median_probability)\n",
        "\n",
        "# # Continue: (9 * overall_accuracy * diff) / 3\n",
        "# # uncertainty = (diff.multiply(oa).multiply(nine)).divide(three)\n",
        "# uncertainty = diff.divide(oa)\n",
        "\n",
        "# # Add the uncertainty as a new band\n",
        "# uncertainty_pre = image.addBands(uncertainty.rename('uncertainty'))\n",
        "\n",
        "# region = uncertainty_pre.geometry()\n",
        "# # Calculate min and max of the uncertainty band\n",
        "# stats = uncertainty_pre.select('uncertainty').reduceRegion(\n",
        "#     reducer=ee.Reducer.minMax(),\n",
        "#     geometry=region,\n",
        "#     scale=3,\n",
        "#     maxPixels=1e9\n",
        "# )\n",
        "\n",
        "# # Get the results\n",
        "# min_val = stats.get('uncertainty_min').getInfo()\n",
        "# max_val = stats.get('uncertainty_max').getInfo()\n",
        "\n",
        "# print('Uncertainty range:', min_val, 'to', max_val)\n",
        "\n",
        "\n",
        "# # ###########################\n",
        "\n",
        "# # Select the uncertainty band\n",
        "# uncertainty_pre = uncertainty_pre.select('uncertainty')\n",
        "\n",
        "# # Normalize to 0â1\n",
        "# normalized_uncertainty = uncertainty.subtract(min_val).divide(max_val - min_val)\n",
        "\n",
        "# # Add it back to the image\n",
        "# overlap_mask = combined_classification_pre.mask().reduce(ee.Reducer.anyNonZero())\n",
        "# uncertainty_pre_norm = image.addBands(normalized_uncertainty.rename('uncertainty_norm'))\n",
        "\n",
        "# region = uncertainty_pre_norm.geometry()\n",
        "# # Calculate min and max of the uncertainty band\n",
        "# stats2 = uncertainty_pre_norm.select('uncertainty_norm').reduceRegion(\n",
        "#     reducer=ee.Reducer.minMax(),\n",
        "#     geometry=region,\n",
        "#     scale=3,\n",
        "#     maxPixels=1e9\n",
        "# )\n",
        "\n",
        "# # Get the results\n",
        "# min_val = stats2.get('uncertainty_norm_min').getInfo()\n",
        "# max_val = stats2.get('uncertainty_norm_max').getInfo()\n",
        "\n",
        "# print('Uncertainty range norm:', min_val, 'to', max_val)\n",
        "\n",
        "# # Visualize the mean uncertainty layer\n",
        "# # Define visualization parameters for the uncertainty band\n",
        "# # You may need to adjust the min/max values based on your data range\n",
        "# vis_params = {'bands': ['uncertainty_norm'], 'palette': ['#4d004b', '#4f004d', '#50014e', '#520150', '#540251', '#550253', '#570354', '#580356', '#5a0457', '#5c0459', '#5d055a', '#5f055c', '#61065d', '#62065f', '#640761', '#650762', '#670864', '#690865', '#6a0867', '#6c0968', '#6e096a', '#6f0a6b', '#710a6d', '#730b6e', '#740b70', '#760c71', '#770c73', '#790d75', '#7b0d76', '#7c0e78', '#7e0e79', '#800f7b', '#810f7c', '#81117d', '#81127e', '#82147f', '#821580', '#821781', '#821982', '#831a83', '#831c84', '#831d85', '#831f86', '#832088', '#842289', '#84248a', '#84258b', '#84278c', '#85288d', '#852a8e', '#852b8f', '#852d90', '#852f91', '#863092', '#863293', '#863394', '#863595', '#873696', '#873897', '#873a98', '#873b99', '#873d9a', '#883e9b', '#88409c', '#88419d', '#88439e', '#88449e', '#88459f', '#8947a0', '#8948a0', '#8949a1', '#894ba2', '#894ca2', '#894da3', '#894fa3', '#8950a4', '#8a51a5', '#8a52a5', '#8a54a6', '#8a55a7', '#8a56a7', '#8a58a8', '#8a59a8', '#8a5aa9', '#8b5caa', '#8b5daa', '#8b5eab', '#8b60ac', '#8b61ac', '#8b62ad', '#8b64ad', '#8b65ae', '#8c66af', '#8c68af', '#8c69b0', '#8c6ab1', '#8c6cb1', '#8c6db2', '#8c6eb3', '#8c70b3', '#8c71b4', '#8c72b5', '#8c74b5', '#8c75b6', '#8c76b7', '#8c78b7', '#8c79b8', '#8c7ab8', '#8c7cb9', '#8c7dba', '#8c7eba', '#8c80bb', '#8c81bc', '#8c82bc', '#8c84bd', '#8c85be', '#8c86be', '#8c88bf', '#8c89c0', '#8c8bc0', '#8c8cc1', '#8c8dc2', '#8c8fc2', '#8c90c3', '#8c91c4', '#8c93c4', '#8c94c5', '#8c95c6', '#8c97c6', '#8d98c7', '#8d99c8', '#8e9ac8', '#8f9bc9', '#8f9dc9', '#909eca', '#909fcb', '#91a0cb', '#91a1cc', '#92a3cd', '#92a4cd', '#93a5ce', '#94a6ce', '#94a7cf', '#95a8d0', '#95aad0', '#96abd1', '#96acd2', '#97add2', '#98aed3', '#98b0d3', '#99b1d4', '#99b2d5', '#9ab3d5', '#9ab4d6', '#9bb6d7', '#9cb7d7', '#9cb8d8', '#9db9d9', '#9dbad9', '#9ebcda', '#9fbcda', '#a0bddb', '#a1bedb', '#a2bfdb', '#a3bfdc', '#a4c0dc', '#a5c1dc', '#a6c2dd', '#a7c2dd', '#a8c3de', '#a9c4de', '#aac4de', '#abc5df', '#acc6df', '#adc7e0', '#aec7e0', '#afc8e0', '#b0c9e1', '#b1c9e1', '#b2cae1', '#b3cbe2', '#b4cce2', '#b5cce3', '#b6cde3', '#b7cee3', '#b9cee4', '#bacfe4', '#bbd0e4', '#bcd1e5', '#bdd1e5', '#bed2e6', '#bfd3e6', '#c0d4e6', '#c1d4e7', '#c2d5e7', '#c3d6e8', '#c4d7e8', '#c5d8e9', '#c6d8e9', '#c7d9e9', '#c8daea', '#c9dbea', '#cadbeb', '#cbdceb', '#ccddec', '#cddeec', '#cedfec', '#cfdfed', '#d0e0ed', '#d1e1ee', '#d2e2ee', '#d3e2ef', '#d4e3ef', '#d6e4f0', '#d7e5f0', '#d8e6f0', '#d9e6f1', '#dae7f1', '#dbe8f2', '#dce9f2', '#ddeaf3', '#deeaf3', '#dfebf4', '#e0ecf4', '#e1ecf4', '#e1edf5', '#e2edf5', '#e3eef5', '#e4eef5', '#e4eff6', '#e5eff6', '#e6f0f6', '#e6f0f7', '#e7f1f7', '#e8f1f7', '#e9f2f7', '#e9f2f8', '#eaf3f8', '#ebf3f8', '#ebf4f8', '#ecf4f9', '#edf5f9', '#eef5f9', '#eef6fa', '#eff6fa', '#f0f7fa', '#f1f7fa', '#f1f8fb', '#f2f8fb', '#f3f9fb', '#f3f9fc', '#f4fafc', '#f5fafc', '#f6fbfc', '#f6fbfd', '#f7fcfd'], 'min': 0, 'max': 1}\n",
        "# Map = geemap.Map()\n",
        "# Map.addLayer(uncertainty_pre_norm.select('uncertainty_norm'), vis_params, \"Uncertainty Pre-event\")\n",
        "# Map.centerObject(OSW, 13)\n",
        "# Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RFucqb2i5el"
      },
      "outputs": [],
      "source": [
        "# # Export the FeatureCollection to Google Drive\n",
        "# task = ee.batch.Export.image.toDrive(\n",
        "#     image=uncertainty_pre_norm.toFloat(),\n",
        "#     description='uncertainty_pre_norm',  # Description for the task\n",
        "#     folder='Colab_Notebooks/export',\n",
        "#     region = OSW.geometry(),\n",
        "#     scale=3,  # Adjust the scale as needed\n",
        "#     fileFormat='GeoTIFF'  # Choose your desired file format (CSV, GeoJSON, KML, SHP, TFRecord)\n",
        "# )\n",
        "\n",
        "# # Start the export task\n",
        "# task.start()\n",
        "\n",
        "# print('Export task started. You can monitor its progress in the \"Tasks\" tab.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRMNsPGGEauF"
      },
      "outputs": [],
      "source": [
        "# ## Calculate uncertainty\n",
        "# # probability IC: probabilities\n",
        "# # classified IC: classification_images_pre\n",
        "# # validation pre: v_FC\n",
        "\n",
        "# def calculate_uncertainty(prob_image, classified_image, validation_fc):\n",
        "#     # Get the probability bands from the probability image\n",
        "#     probabilities_image = prob_image.select(['seagrass', 'nonSeagrass'])\n",
        "\n",
        "#     # Sample the classified image with validation points\n",
        "#     validated = classified_image.sampleRegions(\n",
        "#         collection=validation_fc, # Using the provided validation FeatureCollection\n",
        "#         properties=['habitat'], # Assuming 'habitat' is the class property in v_FC\n",
        "#         scale=3,\n",
        "#         geometries=True\n",
        "#     )\n",
        "\n",
        "#     # Calculate overall accuracy if there are enough validation points\n",
        "#     overall_accuracy = ee.Algorithms.If(\n",
        "#         validated.size().gt(1),\n",
        "#         validated.errorMatrix('habitat', 'classification').accuracy(),\n",
        "#         ee.Number(1.0) # Default accuracy if not enough points\n",
        "#     )\n",
        "\n",
        "#     # Ensure overall_accuracy is treated as an Earth Engine Number\n",
        "#     overall_accuracy = ee.Number(overall_accuracy)\n",
        "\n",
        "#     # Calculate uncertainty per pixel: (1 - Probability of Seagrass) / Overall Accuracy\n",
        "#     # Ensure overall_accuracy is treated as a constant image for pixel-wise division\n",
        "#     overall_accuracy_image = ee.Image.constant(overall_accuracy).float()\n",
        "\n",
        "#     # Calculate the uncertainty based on seagrass probability\n",
        "#     # Lower probability of seagrass indicates higher uncertainty for seagrass class\n",
        "#     # A common approach is 1 - probability or using entropy. Let's use 1 - probability as a simple measure.\n",
        "#     uncertainty_prob = probabilities_image.select('seagrass').multiply(-1).add(1).rename('uncertainty_prob')\n",
        "\n",
        "#     # You could also consider scaling by accuracy, e.g., uncertainty_prob / overall_accuracy\n",
        "#     uncertainty = uncertainty_prob.divide(overall_accuracy_image).rename('uncertainty')\n",
        "\n",
        "#     # Return the uncertainty band, aligning it with the classified image properties\n",
        "#     return classified_image.addBands(uncertainty).copyProperties(classified_image, classified_image.propertyNames())\n",
        "\n",
        "\n",
        "# # Apply the function to your ImageCollections\n",
        "# # We need to iterate through one collection (e.g., classification_images_pre) and find the corresponding image in the other (probabilities)\n",
        "# def map_uncertainty_calculation(classified_image):\n",
        "#     # Find the corresponding probability image using the system:index\n",
        "#     prob_image = probabilities.filter(ee.Filter.eq('system:index', classified_image.get('system:index'))).first()\n",
        "\n",
        "#     # Calculate uncertainty for this image pair\n",
        "#     return calculate_uncertainty(prob_image, classified_image, v_FC)\n",
        "\n",
        "\n",
        "# # Assuming 'classification_images_pre' is your pre-event ImageCollection containing the 'classification' band\n",
        "# # Assuming 'probabilities' is your pre-event ImageCollection containing 'seagrass' and 'nonSeagrass' bands\n",
        "# # Assuming 'v_FC' is your FeatureCollection of validation points (pre-event validation data)\n",
        "\n",
        "# uncertainty_images_pre = soft_map.map(map_uncertainty_calculation())\n",
        "\n",
        "# # You can now visualize or export the 'uncertainty' band from the uncertainty_images_pre collection\n",
        "# print('Uncertainty calculation added to pre-event images.')\n",
        "\n",
        "# # uncertainty_images_pre_mean = uncertainty_images_pre.mean()\n",
        "# # uncertainty_images_pre_mean\n",
        "\n",
        "# uncertainty_images_pre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV8sZjcRxr3L"
      },
      "outputs": [],
      "source": [
        "# # Display the combined classification\n",
        "# Map.addLayer(uncertainty_images_pre.first().select('uncertainty'), {'min': 0, 'max': 1, 'palette': ['42762f', '7cb75a', 'e2e9ab', 'ffffff']}, 'Uncertainty')\n",
        "# Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c01530d1"
      },
      "outputs": [],
      "source": [
        "# create a binary mask: 1 = seagrass, 0 = not\n",
        "# seagrass_mask = final_classification_pre.eq(0).rename('mask')  # change eq(0) if your seagrass label differs\n",
        "\n",
        "# # count seagrass pixels (sum of 1s)\n",
        "# pixel_count = seagrass_mask.reduceRegion(\n",
        "#     reducer=ee.Reducer.sum(),\n",
        "#     geometry=OSW.geometry(),\n",
        "#     scale=3, # 3 m pixels\n",
        "#     maxPixels=1e13\n",
        "# ).get('mask').getInfo()\n",
        "\n",
        "# # convert to hectares: each pixel = 9 m2 -> 9/10000 = 0.0009 ha\n",
        "# area_ha = pixel_count * 9.0 / 10000.0\n",
        "# print(f\"Seagrass pixels: {pixel_count}, area: {area_ha} ha\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73YN-trEvpKp"
      },
      "source": [
        "## 6.2 - Post-event Multi-sensor Image Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu5k8BKDQ7ji"
      },
      "outputs": [],
      "source": [
        "# Binary classification setup\n",
        "seagrass_class = 0\n",
        "nonSeagrass_class = 1\n",
        "\n",
        "# Classification and validation parameters\n",
        "trees = 50\n",
        "t = 45\n",
        "\n",
        "# Boxcar kernel: a square matrix where all values are equal, and it calculates the mean of the pixel values within that square neighborhood.\n",
        "boxcar = ee.Kernel.square(radius=2, units='pixels', normalize=True)\n",
        "bands = multi_sensor_post.first().bandNames()\n",
        "# print(bands.getInfo())\n",
        "\n",
        "def boxcar_image(image):\n",
        "    return image.convolve(boxcar)\n",
        "\n",
        "# Apply the function to every image in the collection\n",
        "classificationComp_post = multi_sensor_post.map(boxcar_image)\n",
        "classificationComp_post"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4ETaRlTRH9R"
      },
      "source": [
        "#### 6.2.1 - Prepare training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICvx1UFBRmKZ"
      },
      "outputs": [],
      "source": [
        "# Training data - class property recoding and merging - MODEL 2 (POST-EVENT)\n",
        "t_seagrass = t_seagrass_post.map(lambda x: x.setMulti(ee.Dictionary.fromLists(['habitat'], [seagrass_class])))\n",
        "t_nonSeagrass = t_nonSeagrass_post.map(lambda x: x.setMulti(ee.Dictionary.fromLists(['habitat'], [nonSeagrass_class])))\n",
        "t_FC = t_seagrass.merge(t_nonSeagrass)\n",
        "\n",
        "\n",
        "# Collect sampled features from each image\n",
        "def sample_image(image):\n",
        "    sampled = image.sampleRegions(\n",
        "        collection=t_FC,\n",
        "        scale=3,\n",
        "        geometries=True\n",
        "    ).filter(ee.Filter.notNull(bands))\n",
        "\n",
        "    return sampled\n",
        "\n",
        "# Map the sampling function and convert result to a list of FeatureCollections\n",
        "sampled_fc_list = classificationComp_post.map(sample_image).toList(classificationComp_post.size())\n",
        "\n",
        "# Flatten the FeatureCollections into one big FeatureCollection\n",
        "sampledData = ee.FeatureCollection(sampled_fc_list).flatten()\n",
        "\n",
        "# Using sampleRegions we take values across the Image Collection. getting valid FeatureCollection when overlapping with pixels\n",
        "# print('sampledData:', sampledData.size().getInfo())\n",
        "print('t_FC:', t_FC.size().getInfo())\n",
        "\n",
        "print('Size of t_seagrass_post:', t_seagrass_post.size().getInfo())\n",
        "print('Size of t_nonSeagrass_post:', t_nonSeagrass_post.size().getInfo())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alSxDTlUSv8T"
      },
      "source": [
        "#### 6.2.2 - Extract probabilities for each class and train classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AInGyhZZTF7z"
      },
      "outputs": [],
      "source": [
        "## To extract feature importance and evaluate\n",
        "def soft_prob_subfn(image, num):\n",
        "       training = sampledData.map(lambda ft: ft.set(\n",
        "        'prob',\n",
        "        ee.Algorithms.If(ft.getNumber('habitat').eq(num), 1, 0)\n",
        "        ))\n",
        "\n",
        "       trained = ee.Classifier.smileRandomForest(numberOfTrees=trees) \\\n",
        "           .train(training, 'prob', image.bandNames()) \\\n",
        "           .setOutputMode('PROBABILITY')\n",
        "\n",
        "       dict_classifier = trained.explain()\n",
        "\n",
        "       # Create a Feature with the dictionary as a property\n",
        "       feature = ee.Feature(None, {'classifier_explanation': dict_classifier})\n",
        "\n",
        "       # Classify and convert to percentage scale\n",
        "       classified = image.classify(trained).multiply(100).toInt8()\n",
        "\n",
        "       # Return both the classified image and the Feature with explanation\n",
        "       return ee.Image(classified).set('classifier_explanation', feature)\n",
        "\n",
        "# Map the function to your ImageCollection\n",
        "seagrass_prob_IC = classificationComp_post.map(lambda img: soft_prob_subfn(img, seagrass_class))\n",
        "nonSeagrass_prob_IC = classificationComp_post.map(lambda img: soft_prob_subfn(img, nonSeagrass_class))\n",
        "\n",
        "### MEAN FEATURE IMPORTANCE\n",
        "# Get the first three images from the ImageCollection\n",
        "first_three_images = seagrass_prob_IC.toList(3)\n",
        "\n",
        "# Initialize an empty dictionary to store feature importances\n",
        "mean_importance = {}\n",
        "\n",
        "# Iterate through the first three images\n",
        "for i in range(3):\n",
        "  image = ee.Image(first_three_images.get(i))\n",
        "  classifier_explanation = image.get('classifier_explanation').getInfo()['properties']['classifier_explanation']\n",
        "  importance_dict = classifier_explanation.get('importance')\n",
        "\n",
        "  # Accumulate feature importances\n",
        "  for feature, importance in importance_dict.items():\n",
        "    mean_importance[feature] = mean_importance.get(feature, 0) + importance\n",
        "\n",
        "# Calculate the mean feature importance\n",
        "for feature in mean_importance:\n",
        "  mean_importance[feature] /= 3\n",
        "\n",
        "# Print the mean feature importance\n",
        "print(mean_importance)\n",
        "\n",
        "# Sort features by importance in descending order\n",
        "sorted_importance = dict(sorted(mean_importance.items(), key=lambda item: item[1], reverse=False))\n",
        "\n",
        "# Extract feature names and importance values\n",
        "feature_names = list(sorted_importance.keys())\n",
        "importance_values = list(sorted_importance.values())\n",
        "\n",
        "# Create a list of colors based on feature name prefix\n",
        "colors = ['#005b96' if name.startswith('PS_') else '#6497b1' for name in feature_names]\n",
        "\n",
        "# Create the horizontal bar graph (switched axes)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_names, importance_values, color=colors)\n",
        "plt.ylabel(\"Features\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.title(\"Feature Importance (2024)\")\n",
        "plt.show()\n",
        "plt.tight_layout()\n",
        "\n",
        "selected_bands = [band for band, importance in mean_importance.items() if importance >= 0]\n",
        "\n",
        "print(\"Selected bands:\", selected_bands)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUXO4j2mAJVT"
      },
      "outputs": [],
      "source": [
        "## Co-variance analysis\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to extract band values from a FeatureCollection\n",
        "def extract_band_values(feature_collection, bands):\n",
        "    data = feature_collection.toList(feature_collection.size()).map(lambda feature: ee.Feature(feature).toDictionary().select(bands)).getInfo()\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "band_values_df = extract_band_values(sampledData, selected_bands)\n",
        "\n",
        "correlation_matrix = band_values_df.corr()\n",
        "\n",
        "# Create a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(correlation_matrix, bool), k=1)\n",
        "\n",
        "# Apply the mask to the filtered correlation matrix\n",
        "filtered_correlation_matrix = correlation_matrix.mask(mask)\n",
        "filtered_correlation_matrix = filtered_correlation_matrix[np.abs(correlation_matrix) > 0.80]\n",
        "\n",
        "# Get bands with at least one correlation above 0.9\n",
        "bands_to_keep = filtered_correlation_matrix.columns[filtered_correlation_matrix.any()]\n",
        "\n",
        "# Filter the original correlation matrix to keep only relevant bands\n",
        "filtered_correlation_matrix = correlation_matrix.loc[bands_to_keep, bands_to_keep]\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(filtered_correlation_matrix, mask=mask, annot=True, cmap='coolwarm',\n",
        "            fmt=\".2f\", linewidths=.5, linecolor='lightgrey', square=True,\n",
        "            cbar_kws={\"shrink\": .75})\n",
        "plt.title('Band Covariability Analysis')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Bands to keep:\", bands_to_keep.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7vpsYmmAVq_"
      },
      "source": [
        "### 6.2.3 - Generate probability layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RL1u5RSWAYKE"
      },
      "outputs": [],
      "source": [
        "## Train classifier and display probability map for seagrasses\n",
        "# Define classes dictionary\n",
        "classes = {\n",
        "  'classes_values': [seagrass_class, nonSeagrass_class],\n",
        "  'classes_names': ['seagrass','nonSeagrass']\n",
        "}\n",
        "\n",
        "classes\n",
        "\n",
        "def classify_with_all_classes(image):\n",
        "    # Classify image for each class and rename the result accordingly\n",
        "    classified_images = ee.List(classes['classes_values']).map(\n",
        "        lambda class_id: soft_prob_subfn(image, class_id)\n",
        "    )\n",
        "\n",
        "    # Convert list of images into one multi-band image (each class as band)\n",
        "    classified_combined = ee.ImageCollection(classified_images).toBands()\n",
        "    classified_combined = classified_combined.rename(classes['classes_names'])\n",
        "\n",
        "    # Preserve metadata if needed\n",
        "    return classified_combined.copyProperties(image, image.propertyNames())\n",
        "\n",
        "# Generate probabilities in the ImageCollection\n",
        "probabilities_post = classificationComp_post.map(classify_with_all_classes)\n",
        "probabilities_post"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMhgfl_IAqVR"
      },
      "outputs": [],
      "source": [
        "# Visualize probabilities\n",
        "first_prob = ee.Image(probabilities_post.toList(probabilities_post.size()).get(0))\n",
        "Map = geemap.Map()\n",
        "Map.addLayer(first_prob.select('seagrass'), {'min': 45, 'max': 100}, 'Seagrass Prob')\n",
        "Map.addLayer(first_prob.select('nonSeagrass'), {'min': 45, 'max': 100}, 'NonSeagrass Prob')\n",
        "Map.centerObject(probabilities_post.first(), zoom=12)\n",
        "Map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IaIhvZdA57j"
      },
      "source": [
        "#### 6.2.4 - Threshold classes based on probabilities and classify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQcO9zFYA6ue"
      },
      "outputs": [],
      "source": [
        "# Based on the generated probabilities, apply thresholding for seagrass and nonSeagrass on the whole Image Collection\n",
        "\n",
        "# Threshold function for seagrass\n",
        "def threshold_seagrass(image):\n",
        "    return image.select('seagrass').updateMask(image.select('seagrass').gt(t)) \\\n",
        "        .copyProperties(image, image.propertyNames())\n",
        "\n",
        "# Threshold function for non-seagrass\n",
        "def threshold_non_seagrass(image):\n",
        "    return image.select('nonSeagrass').updateMask(image.select('nonSeagrass').gt(t)) \\\n",
        "        .copyProperties(image, image.propertyNames())\n",
        "\n",
        "# Map the thresholding over the probabilities collection\n",
        "seagrass_soft = probabilities_post.map(threshold_seagrass)\n",
        "nonSeagrass_soft = probabilities_post.map(threshold_non_seagrass)\n",
        "nonSeagrass_soft\n",
        "\n",
        "# Function to combine seagrass and nonSeagrass into one image\n",
        "def add_bands(image):\n",
        "    matching_non_seagrass = nonSeagrass_soft.filter(\n",
        "        ee.Filter.eq('system:index', image.get('system:index'))\n",
        "    ).first()\n",
        "\n",
        "    # If no match is found, return image as-is\n",
        "    combined = ee.Image(image).addBands(ee.Image(matching_non_seagrass))\n",
        "    return combined.copyProperties(image, image.propertyNames())\n",
        "\n",
        "# Map the function to seagrass_soft collection to create the aggregated collection\n",
        "soft_map = seagrass_soft.map(add_bands)\n",
        "soft_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEEXi_o5BQeA"
      },
      "source": [
        "#### 6.2.5 - Perform classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBgfKy7xBTIj"
      },
      "outputs": [],
      "source": [
        "def soft_to_hard(image):\n",
        "    seagrass = image.select('seagrass')\n",
        "    non_seagrass = image.select('nonSeagrass')\n",
        "    prob_aoi = ee.Image(probabilities_post.first()).geometry()\n",
        "\n",
        "    # Get the corresponding images from seagrass_soft and nonSeagrass_soft using system:index\n",
        "    seagrass_soft_image = seagrass_soft.filter(ee.Filter.eq('system:index', image.get('system:index'))).first()\n",
        "    nonSeagrass_soft_image = nonSeagrass_soft.filter(ee.Filter.eq('system:index', image.get('system:index'))).first()\n",
        "\n",
        "    classified = (ee.Image.constant(2)\n",
        "        .where(seagrass.gte(t), ee.Number(0).add(1))  # temporary +1 to mask 0s\n",
        "        .where(non_seagrass.gte(t), ee.Number(1).add(1))\n",
        "        .selfMask()\n",
        "        .subtract(1)  # back to 0 and 1\n",
        "        .rename('classification')\n",
        "        .clip(prob_aoi))\n",
        "\n",
        "    return classified.copyProperties(image, image.propertyNames())\n",
        "\n",
        "# Apply to soft_map\n",
        "soft_map_th = soft_map.map(soft_to_hard)\n",
        "soft_map_th"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXt9RFUYBbv0"
      },
      "source": [
        "#### 6.2.6 - Combine the three classifications for visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IOSgVQABdfg"
      },
      "outputs": [],
      "source": [
        "# 1. Extract the classification band from each image\n",
        "classification_images_post = soft_map_th.select('classification')\n",
        "\n",
        "# 2. Calculate the mean value\n",
        "combined_classification_post = classification_images_post.mean().mask(land_mask).clip(OSW)\n",
        "# print(combined_classification.getInfo())\n",
        "\n",
        "Map.addLayer(combined_classification_post, {'min': 0, 'max': 1, 'palette': ['42762f', '7cb75a', 'e2e9ab', 'ffffff']}, 'Combined Classification')\n",
        "Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psE8WRprrPkZ"
      },
      "outputs": [],
      "source": [
        "# # Export the FeatureCollection to Google Drive\n",
        "# task = ee.batch.Export.image.toDrive(\n",
        "#     image=combined_classification_post,\n",
        "#     description='combined_classification_post',  # Description for the task\n",
        "#     folder='Colab_Notebooks/export',\n",
        "#     region = OSW.geometry(),\n",
        "#     scale=3,  # Adjust the scale as needed\n",
        "#     fileFormat='GeoTIFF'  # Choose your desired file format (CSV, GeoJSON, KML, SHP, TFRecord)\n",
        "# )\n",
        "\n",
        "# # Start the export task\n",
        "# task.start()\n",
        "\n",
        "# print('Export task started. You can monitor its progress in the \"Tasks\" tab.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMX-s1IbIACl"
      },
      "outputs": [],
      "source": [
        "# Apply conditional classification for obtain the final binary map\n",
        "final_classification_post = combined_classification_post.expression(\n",
        "    'b(0) < 0.35 ? 0 : 1',  # Conditional expression\n",
        "    {'classification': combined_classification_post}  # Band name mapping\n",
        ").rename('classification')  # Rename the band\n",
        "\n",
        "# Display the combined classification\n",
        "Map.addLayer(final_classification_post.mask(land_mask).clip(OSW.geometry()), {'min': 0, 'max': 1, 'palette': ['42762f', 'ffffff']}, 'Combined Classification')\n",
        "Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqhdfPvRTFlN"
      },
      "outputs": [],
      "source": [
        "final_classification_post"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnb-3cgaCDuK"
      },
      "source": [
        "#### 6.2.7 - Validation and accuracy assessment for each of the 3 classifications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9upFEQXFCFdJ"
      },
      "outputs": [],
      "source": [
        "# Validation function\n",
        "def valid_habitat(number):\n",
        "  def wrap(feature):\n",
        "    return feature.setMulti(ee.Dictionary.fromLists(['habitat'], [number]))\n",
        "  return wrap\n",
        "\n",
        "## Validation data\n",
        "v_seagrass = v_seagrass_post.map(valid_habitat(seagrass_class))\n",
        "v_nonSeagrass = v_nonSeagrass_post.map(valid_habitat(nonSeagrass_class))\n",
        "v_FC = v_seagrass.merge(v_nonSeagrass)\n",
        "v_FC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bKSqFQRDWSB"
      },
      "outputs": [],
      "source": [
        "def validation(image):\n",
        "    sampled = image.sampleRegions(\n",
        "        collection=v_FC,\n",
        "        properties=['habitat'],\n",
        "        scale=3)\n",
        "    return sampled\n",
        "\n",
        "# Map the sampling function and convert result to a list of FeatureCollections\n",
        "sampled_v_list = soft_map_th.map(validation).toList(soft_map_th.size())\n",
        "\n",
        "# Flatten the FeatureCollections into one big FeatureCollection\n",
        "sampledValidation_post = ee.FeatureCollection(sampled_v_list).flatten()\n",
        "\n",
        "# Using sampleRegions we take values across the Image Collection. getting valid FeatureCollection when overlapping with pixels\n",
        "print('sampled Validation:', sampledValidation_post.size().getInfo())\n",
        "sampledValidation_post"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8b2ttAvDhOd"
      },
      "outputs": [],
      "source": [
        "errorMatrix = sampledValidation_post.errorMatrix('habitat', 'classification')\n",
        "print('Confusion Matrix:', errorMatrix.getInfo())\n",
        "\n",
        "confusion_matrix_data = errorMatrix.array().getInfo()\n",
        "\n",
        "# Convert the list to a NumPy array\n",
        "matrix_values = np.array(confusion_matrix_data)  # Convert to NumPy array\n",
        "\n",
        "# Define class labels\n",
        "class_names = ['seagrass', 'nonSeagrass']\n",
        "\n",
        "# Create ConfusionMatrixDisplay object\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=matrix_values, display_labels=class_names)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel(\"EO prediction\", fontsize = 14)\n",
        "plt.ylabel(\"In situ data\", fontsize = 14)\n",
        "disp.ax_.tick_params(axis='both', which='major', labelsize=18)\n",
        "plt.show()\n",
        "\n",
        "overall_accuracy = errorMatrix.accuracy().getInfo()\n",
        "print('Overall Accuracy:', overall_accuracy)\n",
        "producers_accuracy = ee.Array(errorMatrix.producersAccuracy()).reshape([-1]).getInfo()\n",
        "print('PA:', producers_accuracy)\n",
        "users_accuracy = ee.Array(errorMatrix.consumersAccuracy()).reshape([-1]).getInfo()\n",
        "print('UA:', users_accuracy)\n",
        "\n",
        "# Calcuate F1-score\n",
        "truePositives = errorMatrix.array().get([0, 0])\n",
        "falsePositives = errorMatrix.array().get([1, 0])\n",
        "falseNegatives = errorMatrix.array().get([0, 1])\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = truePositives.divide(truePositives.add(falsePositives))\n",
        "recall = truePositives.divide(truePositives.add(falseNegatives))\n",
        "f1_score = ee.Number(2).multiply(precision.multiply(recall)).divide(precision.add(recall))\n",
        "\n",
        "print('F1-score:', f1_score.getInfo())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJK-_1H0D2RX"
      },
      "source": [
        "#### 6.1.8 - Uncertainty analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lFIGzvT8DOh"
      },
      "outputs": [],
      "source": [
        "## To run uncertainties in the first classification (pre-event, 2018)\n",
        "## t=(9*OA*(3-median probabilities))/3\n",
        "# 9 = area of pixel in square meters (3x3)\n",
        "# 3 = number of images\n",
        "\n",
        "## first scale probabilities to 0-1 values\n",
        "def scale_to_unit(image):\n",
        "    return image.divide(100).copyProperties(image, image.propertyNames())\n",
        "\n",
        "# Apply the scaling function to the collection\n",
        "probabilities_01 = probabilities.map(scale_to_unit)\n",
        "\n",
        "median_probability = probabilities_01.select('seagrass').median().rename('seagrass_median_prob')\n",
        "median_probability\n",
        "\n",
        "# Select the band containing median probability\n",
        "median_probability = median_probability.select('seagrass_median_prob')\n",
        "\n",
        "# Apply the formula:\n",
        "# uncertainty = (9 * overall_accuracy * (3 - median_probability)) / 3\n",
        "three = ee.Number(3)\n",
        "nine = ee.Number(9)\n",
        "oa = ee.Number(overall_accuracy)\n",
        "\n",
        "# Compute: (3 - median_prob)\n",
        "diff = ee.Image.constant(1).subtract(median_probability)\n",
        "\n",
        "# Continue: (9 * overall_accuracy * diff) / 3\n",
        "# uncertainty = (diff.multiply(oa).multiply(nine)).divide(three)\n",
        "uncertainty = diff.divide(oa)\n",
        "\n",
        "# Add the uncertainty as a new band\n",
        "uncertainty_post = image.addBands(uncertainty.rename('uncertainty'))\n",
        "\n",
        "region = uncertainty_post.geometry()\n",
        "# Calculate min and max of the uncertainty band\n",
        "stats = uncertainty_post.select('uncertainty').reduceRegion(\n",
        "    reducer=ee.Reducer.minMax(),\n",
        "    geometry=region,\n",
        "    scale=3,\n",
        "    maxPixels=1e9\n",
        ")\n",
        "\n",
        "# Get the results\n",
        "min_val = stats.get('uncertainty_min').getInfo()\n",
        "max_val = stats.get('uncertainty_max').getInfo()\n",
        "\n",
        "print('Uncertainty range:', min_val, 'to', max_val)\n",
        "\n",
        "\n",
        "# ###########################\n",
        "\n",
        "# Select the uncertainty band\n",
        "uncertainty_post = uncertainty_post.select('uncertainty')\n",
        "\n",
        "# Normalize to 0â1\n",
        "normalized_uncertainty = uncertainty_post.subtract(min_val).divide(max_val - min_val)\n",
        "\n",
        "# Add it back to the image\n",
        "overlap_mask = combined_classification_post.mask().reduce(ee.Reducer.anyNonZero())\n",
        "uncertainty_post_norm = image.addBands(normalized_uncertainty.rename('uncertainty_norm'))\n",
        "\n",
        "region = uncertainty_pre_norm.geometry()\n",
        "# Calculate min and max of the uncertainty band\n",
        "stats2 = uncertainty_post_norm.select('uncertainty_norm').reduceRegion(\n",
        "    reducer=ee.Reducer.minMax(),\n",
        "    geometry=region,\n",
        "    scale=3,\n",
        "    maxPixels=1e9\n",
        ")\n",
        "\n",
        "# Get the results\n",
        "min_val = stats2.get('uncertainty_norm_min').getInfo()\n",
        "max_val = stats2.get('uncertainty_norm_max').getInfo()\n",
        "\n",
        "print('Uncertainty range norm:', min_val, 'to', max_val)\n",
        "\n",
        "# Visualize the mean uncertainty layer\n",
        "# Define visualization parameters for the uncertainty band\n",
        "# You may need to adjust the min/max values based on your data range\n",
        "vis_params = {'bands': ['uncertainty_norm'], 'palette': ['#4d004b', '#4f004d', '#50014e', '#520150', '#540251', '#550253', '#570354', '#580356', '#5a0457', '#5c0459', '#5d055a', '#5f055c', '#61065d', '#62065f', '#640761', '#650762', '#670864', '#690865', '#6a0867', '#6c0968', '#6e096a', '#6f0a6b', '#710a6d', '#730b6e', '#740b70', '#760c71', '#770c73', '#790d75', '#7b0d76', '#7c0e78', '#7e0e79', '#800f7b', '#810f7c', '#81117d', '#81127e', '#82147f', '#821580', '#821781', '#821982', '#831a83', '#831c84', '#831d85', '#831f86', '#832088', '#842289', '#84248a', '#84258b', '#84278c', '#85288d', '#852a8e', '#852b8f', '#852d90', '#852f91', '#863092', '#863293', '#863394', '#863595', '#873696', '#873897', '#873a98', '#873b99', '#873d9a', '#883e9b', '#88409c', '#88419d', '#88439e', '#88449e', '#88459f', '#8947a0', '#8948a0', '#8949a1', '#894ba2', '#894ca2', '#894da3', '#894fa3', '#8950a4', '#8a51a5', '#8a52a5', '#8a54a6', '#8a55a7', '#8a56a7', '#8a58a8', '#8a59a8', '#8a5aa9', '#8b5caa', '#8b5daa', '#8b5eab', '#8b60ac', '#8b61ac', '#8b62ad', '#8b64ad', '#8b65ae', '#8c66af', '#8c68af', '#8c69b0', '#8c6ab1', '#8c6cb1', '#8c6db2', '#8c6eb3', '#8c70b3', '#8c71b4', '#8c72b5', '#8c74b5', '#8c75b6', '#8c76b7', '#8c78b7', '#8c79b8', '#8c7ab8', '#8c7cb9', '#8c7dba', '#8c7eba', '#8c80bb', '#8c81bc', '#8c82bc', '#8c84bd', '#8c85be', '#8c86be', '#8c88bf', '#8c89c0', '#8c8bc0', '#8c8cc1', '#8c8dc2', '#8c8fc2', '#8c90c3', '#8c91c4', '#8c93c4', '#8c94c5', '#8c95c6', '#8c97c6', '#8d98c7', '#8d99c8', '#8e9ac8', '#8f9bc9', '#8f9dc9', '#909eca', '#909fcb', '#91a0cb', '#91a1cc', '#92a3cd', '#92a4cd', '#93a5ce', '#94a6ce', '#94a7cf', '#95a8d0', '#95aad0', '#96abd1', '#96acd2', '#97add2', '#98aed3', '#98b0d3', '#99b1d4', '#99b2d5', '#9ab3d5', '#9ab4d6', '#9bb6d7', '#9cb7d7', '#9cb8d8', '#9db9d9', '#9dbad9', '#9ebcda', '#9fbcda', '#a0bddb', '#a1bedb', '#a2bfdb', '#a3bfdc', '#a4c0dc', '#a5c1dc', '#a6c2dd', '#a7c2dd', '#a8c3de', '#a9c4de', '#aac4de', '#abc5df', '#acc6df', '#adc7e0', '#aec7e0', '#afc8e0', '#b0c9e1', '#b1c9e1', '#b2cae1', '#b3cbe2', '#b4cce2', '#b5cce3', '#b6cde3', '#b7cee3', '#b9cee4', '#bacfe4', '#bbd0e4', '#bcd1e5', '#bdd1e5', '#bed2e6', '#bfd3e6', '#c0d4e6', '#c1d4e7', '#c2d5e7', '#c3d6e8', '#c4d7e8', '#c5d8e9', '#c6d8e9', '#c7d9e9', '#c8daea', '#c9dbea', '#cadbeb', '#cbdceb', '#ccddec', '#cddeec', '#cedfec', '#cfdfed', '#d0e0ed', '#d1e1ee', '#d2e2ee', '#d3e2ef', '#d4e3ef', '#d6e4f0', '#d7e5f0', '#d8e6f0', '#d9e6f1', '#dae7f1', '#dbe8f2', '#dce9f2', '#ddeaf3', '#deeaf3', '#dfebf4', '#e0ecf4', '#e1ecf4', '#e1edf5', '#e2edf5', '#e3eef5', '#e4eef5', '#e4eff6', '#e5eff6', '#e6f0f6', '#e6f0f7', '#e7f1f7', '#e8f1f7', '#e9f2f7', '#e9f2f8', '#eaf3f8', '#ebf3f8', '#ebf4f8', '#ecf4f9', '#edf5f9', '#eef5f9', '#eef6fa', '#eff6fa', '#f0f7fa', '#f1f7fa', '#f1f8fb', '#f2f8fb', '#f3f9fb', '#f3f9fc', '#f4fafc', '#f5fafc', '#f6fbfc', '#f6fbfd', '#f7fcfd'], 'min': 0, 'max': 1}\n",
        "Map = geemap.Map()\n",
        "Map.addLayer(uncertainty_post_norm.select('uncertainty_norm'), vis_params, \"Uncertainty Post-event\")\n",
        "Map.centerObject(OSW, 13)\n",
        "Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IG5EHY5orJCm"
      },
      "outputs": [],
      "source": [
        "# # Export the FeatureCollection to Google Drive\n",
        "# task = ee.batch.Export.image.toDrive(\n",
        "#     image=uncertainty_post_norm.toFloat(),\n",
        "#     description='uncertainty_post_norm',  # Description for the task\n",
        "#     folder='Colab_Notebooks/export',\n",
        "#     region = OSW.geometry(),\n",
        "#     scale=3,  # Adjust the scale as needed\n",
        "#     fileFormat='GeoTIFF'  # Choose your desired file format (CSV, GeoJSON, KML, SHP, TFRecord)\n",
        "# )\n",
        "\n",
        "# # Start the export task\n",
        "# task.start()\n",
        "\n",
        "# print('Export task started. You can monitor its progress in the \"Tasks\" tab.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E1amS9ub-Bn"
      },
      "source": [
        "### Area quantification and +-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZL_GJoUW5WgL"
      },
      "outputs": [],
      "source": [
        "# Mask seagrass pixels\n",
        "seagrass_post = final_classification_post.eq(0)\n",
        "\n",
        "# create a binary mask: 1 = seagrass, 0 = not\n",
        "seagrass_mask = seagrass_post.rename('seagrass')  # change eq(0) if your seagrass label differs\n",
        "\n",
        "# count seagrass pixels (sum of 1s)\n",
        "pixel_count = seagrass_mask.reduceRegion(\n",
        "    reducer=ee.Reducer.sum(),\n",
        "    geometry=OSW.geometry(),\n",
        "    scale=3, # 3 m pixels\n",
        "    maxPixels=1e13\n",
        ").get('seagrass').getInfo()\n",
        "\n",
        "# convert to hectares: each pixel = 9 m2 -> 9/10000 = 0.0009 ha\n",
        "area_ha_post = pixel_count * 9.0 / 10000.0\n",
        "\n",
        "print(f\"Seagrass pixels: {pixel_count}, area: {area_ha_post} ha\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_1O8ugydcTT"
      },
      "outputs": [],
      "source": [
        "# Scale probabilities from 0-100 to 0-1\n",
        "probabilities_post = probabilities_post.map(lambda image: image.divide(100))\n",
        "\n",
        "# Sum the seagrass probabilities for pre-event images\n",
        "sum_seagrass_probabilities_post = probabilities_post.select('seagrass').sum()\n",
        "sum_seagrass_probabilities_post\n",
        "\n",
        "# Calculate the mean of the summed probabilities within the seagrass mask\n",
        "mean_sum_prob_seagrass_post = sum_seagrass_probabilities_post.updateMask(seagrass_mask).reduceRegion(\n",
        "    reducer=ee.Reducer.mean(),\n",
        "    geometry=OSW.geometry(),\n",
        "    scale=3, # Use the same scale as classification\n",
        "    maxPixels=1e13\n",
        ").get('seagrass')\n",
        "\n",
        "mean_sum_prob_seagrass_post = ee.Number(mean_sum_prob_seagrass_post)\n",
        "\n",
        "overall_accuracy_post = ee.Number(overall_accuracy) # Use the overall_accuracy from the pre-event validation cell\n",
        "\n",
        "# Get the total seagrass area for the pre-event classification (assuming it's stored in area_ha variable from cell ZfHt_UQ49oKf)\n",
        "# If not, you might need to recalculate it here or fetch it from the appropriate variable.\n",
        "total_seagrass_area_post = ee.Number(area_ha_post)\n",
        "\n",
        "\n",
        "# Calculate uncertainty for the pre-event seagrass area using the provided formula:\n",
        "# uncertainty = area * Overall Accuracy * (3 - mean of the sum of the probabilities predicted as seagrass) / 3\n",
        "# Assuming '3' is the number of images used in the pre-event period (sentinel2_l1c_pre has 3 images)\n",
        "num_images_post = ee.Number(sentinel2_l1c_post.size()) # Get the actual number of pre-event images\n",
        "\n",
        "uncertainty_post_seagrass_area = total_seagrass_area_post.multiply(overall_accuracy_post).multiply(num_images_post.subtract(mean_sum_prob_seagrass_post)).divide(num_images_post)\n",
        "\n",
        "print(f\"Uncertainty of post-event seagrass area: {area_ha_post} +- {uncertainty_post_seagrass_area.getInfo():.2f} ha\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfYX56gED7Zr"
      },
      "source": [
        "# 7 - Seagrass change detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7JRnYMwY66P"
      },
      "outputs": [],
      "source": [
        "seagrass_pre = final_classification_pre.updateMask(final_classification_pre.select('classification').eq(0))\n",
        "seagrass_post = final_classification_post.updateMask(final_classification_post.select('classification').eq(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GemJi8lncc3a"
      },
      "outputs": [],
      "source": [
        "# Change from 0 to 1 (e.g., newly appeared features)\n",
        "gain = final_classification_pre.eq(1).And(final_classification_post.eq(0)).rename('gain')\n",
        "\n",
        "# Change from 1 to 0 (e.g., disappeared features)\n",
        "loss = final_classification_pre.eq(0).And(final_classification_post.eq(1)).rename('loss')\n",
        "\n",
        "# Stable seagrass meadows\n",
        "stable = final_classification_pre.eq(0).And(final_classification_post.eq(0)).rename('stable')\n",
        "\n",
        "# Combine the conditions into a single image using 'where'\n",
        "seagrass_change = ee.Image(0) \\\n",
        "    .where(loss, 1) \\\n",
        "    .where(stable, 2) \\\n",
        "    .where(gain, 3)\n",
        "\n",
        "seagrass_change\n",
        "gain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSbpFEWa7qkm"
      },
      "outputs": [],
      "source": [
        "# count seagrass pixels (sum of 1s)\n",
        "pixel_count_loss = loss.reduceRegion(\n",
        "    reducer=ee.Reducer.sum(),\n",
        "    geometry=OSW.geometry(),\n",
        "    scale=3, # 3 m pixels\n",
        "    maxPixels=1e13\n",
        ").get('loss').getInfo()\n",
        "\n",
        "# convert to hectares: each pixel = 9 m2 -> 9/10000 = 0.0009 ha\n",
        "area_ha_loss = pixel_count_loss * 9.0 / 10000.0\n",
        "\n",
        "# count seagrass pixels (sum of 1s)\n",
        "pixel_count_gain = gain.reduceRegion(\n",
        "    reducer=ee.Reducer.sum(),\n",
        "    geometry=OSW.geometry(),\n",
        "    scale=3, # 3 m pixels\n",
        "    maxPixels=1e13\n",
        ").get('gain').getInfo()\n",
        "\n",
        "# convert to hectares: each pixel = 9 m2 -> 9/10000 = 0.0009 ha\n",
        "area_ha_gain = pixel_count_gain * 9.0 / 10000.0\n",
        "\n",
        "# count seagrass pixels (sum of 1s)\n",
        "pixel_count_stable = stable.reduceRegion(\n",
        "    reducer=ee.Reducer.sum(),\n",
        "    geometry=OSW.geometry(),\n",
        "    scale=3, # 3 m pixels\n",
        "    maxPixels=1e13\n",
        ").get('stable').getInfo()\n",
        "\n",
        "# convert to hectares: each pixel = 9 m2 -> 9/10000 = 0.0009 ha\n",
        "area_ha_stable = pixel_count_stable * 9.0 / 10000.0\n",
        "\n",
        "print(f\"Seagrass lost: {pixel_count_loss}, area: {area_ha_loss} ha\")\n",
        "print(f\"Seagrass gained: {pixel_count_gain}, area: {area_ha_gain} ha\")\n",
        "print(f\"Seagrass stable: {pixel_count_stable}, area: {area_ha_stable} ha\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FatXcUEE7cwV"
      },
      "outputs": [],
      "source": [
        "num_images_pre = ee.Number(sentinel2_l1c_pre.size()) # Get the actual number of pre-event images\n",
        "\n",
        "# Sum the seagrass probabilities for pre-event images\n",
        "sum_seagrass_probabilities_post = probabilities_post.select('seagrass').sum()\n",
        "sum_seagrass_probabilities_pre = probabilities_pre.select('seagrass').sum()\n",
        "sum_seagrass_probabilities_mean = sum_seagrass_probabilities_pre.add(sum_seagrass_probabilities_post).divide(2)\n",
        "\n",
        "# Calculate the mean of the summed probabilities within the seagrass mask\n",
        "mean_sum_prob_seagrass_mean = sum_seagrass_probabilities_mean.updateMask(gain).reduceRegion(\n",
        "    reducer=ee.Reducer.mean(),\n",
        "    geometry=OSW.geometry(),\n",
        "    scale=3, # Use the same scale as classification\n",
        "    maxPixels=1e13\n",
        ").get('seagrass')\n",
        "\n",
        "mean_sum_prob_seagrass_mean_gain = ee.Number(mean_sum_prob_seagrass_mean)\n",
        "\n",
        "OA_change = overall_accuracy_pre.add(overall_accuracy_post).divide(2)\n",
        "\n",
        "uncertainty_post_seagrass_gain = ee.Number(area_ha_gain).multiply(ee.Number(OA_change)).multiply(num_images_pre.subtract(mean_sum_prob_seagrass_mean_gain)).divide(num_images_pre)\n",
        "\n",
        "print(f\"Uncertainty gain seagrass: {area_ha_gain} +- {uncertainty_post_seagrass_gain.getInfo():.2f} ha\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU8VBdIDFFgJ"
      },
      "outputs": [],
      "source": [
        "# Calculate the mean of the summed probabilities within the seagrass mask\n",
        "mean_sum_prob_seagrass_mean = sum_seagrass_probabilities_mean.updateMask(loss).reduceRegion(\n",
        "    reducer=ee.Reducer.mean(),\n",
        "    geometry=OSW.geometry(),\n",
        "    scale=3, # Use the same scale as classification\n",
        "    maxPixels=1e13\n",
        ").get('seagrass')\n",
        "\n",
        "mean_sum_prob_seagrass_mean_loss = ee.Number(mean_sum_prob_seagrass_mean)\n",
        "\n",
        "uncertainty_post_seagrass_loss = ee.Number(area_ha_loss).multiply(ee.Number(OA_change)).multiply(num_images_pre.subtract(mean_sum_prob_seagrass_mean_loss)).divide(num_images_pre)\n",
        "\n",
        "print(f\"Uncertainty loss seagrass: {area_ha_loss} +- {uncertainty_post_seagrass_loss.getInfo():.2f} ha\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwNBI0ChFcWn"
      },
      "outputs": [],
      "source": [
        "# Calculate the mean of the summed probabilities within the seagrass mask\n",
        "mean_sum_prob_seagrass_mean = sum_seagrass_probabilities_mean.updateMask(stable).reduceRegion(\n",
        "    reducer=ee.Reducer.mean(),\n",
        "    geometry=OSW.geometry(),\n",
        "    scale=3, # Use the same scale as classification\n",
        "    maxPixels=1e13\n",
        ").get('seagrass')\n",
        "\n",
        "mean_sum_prob_seagrass_mean_stable = ee.Number(mean_sum_prob_seagrass_mean)\n",
        "\n",
        "uncertainty_post_seagrass_stable = ee.Number(area_ha_stable).multiply(ee.Number(OA_change)).multiply(num_images_pre.subtract(mean_sum_prob_seagrass_mean_stable)).divide(num_images_pre)\n",
        "\n",
        "print(f\"Uncertainty stable seagrass: {area_ha_stable} +- {mean_sum_prob_seagrass_mean_stable.getInfo():.2f} ha\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ZYnUwfkbglaD",
        "tHJSLzJ2unAe",
        "uzS6h86aF0yr",
        "ilbT_ecVWRGi",
        "upgqHAHL0EZy",
        "lkUkRoVY6Nae",
        "KfYX56gED7Zr"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}